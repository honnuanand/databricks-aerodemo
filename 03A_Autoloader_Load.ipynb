{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8c74af68-1295-4caa-802f-5d6f5989ac72",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Notebook 3: Autoloader-Based Load\n",
    "Use Databricks Autoloader to ingest CSV files from the volume into Delta tables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cf04b947-8cff-45ee-8203-9c71c049eaa0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    " üìò When to Run `03_Autoloader_Load.ipynb` vs. `03B_Autoloader_Maintenance_Events.ipynb`\n",
    "\n",
    "üîÅ `03_Autoloader_Load.ipynb`\n",
    "Use this notebook to ingest **raw sensor data** into the `raw_sensor_data` Delta table.\n",
    "\n",
    "- üìÅ Watches files in: `/Volumes/arao/aerodemo/tmp/raw/`\n",
    "- üìÑ Looks for: `raw_sensor_data_*.csv`\n",
    "- üóÇ Schema: timestamp, aircraft_id, model, engine_temp, fuel_efficiency, vibration\n",
    "- ‚úÖ Run this when:\n",
    "  - You've generated new synthetic raw sensor files\n",
    "  - You want to simulate ingestion of new telemetry data\n",
    "  - You're testing DLT pipelines that depend on `raw_sensor_data`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a02c12a3-1846-417d-a209-5a60b300ce62",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "volume_path = \"/Volumes/arao/aerodemo/tmp/1\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "38f55d62-dc33-455c-a233-08cd60d56c01",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "This setup ensures:\n",
    "\n",
    "- ‚úÖ Only valid CSV files are processed  \n",
    "- ‚úÖ Schema is controlled and predictable  \n",
    "- ‚úÖ Auto Loader correctly tracks ingestion state  \n",
    "- ‚úÖ Safe for batch-mode re-runs via `.trigger(once=True)`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d7d9bdc5-fe7b-4bc9-a776-191b089385ad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "# Define correct schema\n",
    "sensor_schema = StructType([\n",
    "    StructField(\"timestamp\", TimestampType(), True),\n",
    "    StructField(\"aircraft_id\", StringType(), True),\n",
    "    StructField(\"model\", StringType(), True),\n",
    "    StructField(\"engine_temp\", DoubleType(), True),\n",
    "    StructField(\"fuel_efficiency\", DoubleType(), True),\n",
    "    StructField(\"vibration\", DoubleType(), True),\n",
    "    StructField(\"altitude\", DoubleType(), True),\n",
    "    StructField(\"pressure\", DoubleType(), True),\n",
    "    StructField(\"speed\", DoubleType(), True)\n",
    "])\n",
    "\n",
    "# Use dedicated path for raw sensor data files\n",
    "volume_path = \"/Volumes/arao/aerodemo/tmp/raw\"\n",
    "\n",
    "# Define Auto Loader stream\n",
    "raw_df = (spark.readStream\n",
    "    .format(\"cloudFiles\")\n",
    "    .option(\"cloudFiles.format\", \"csv\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"pathGlobFilter\", \"*.csv\")  # Only process CSV files\n",
    "    .option(\"cloudFiles.inferColumnTypes\", \"true\")\n",
    "    .option(\"mergeSchema\", \"true\")\n",
    "    .option(\"cloudFiles.schemaLocation\", f\"{volume_path}/schema/raw_sensor_data\")\n",
    "    .schema(sensor_schema)  # Enforce schema to avoid inference errors\n",
    "    .load(volume_path))\n",
    "\n",
    "# Optional: Preview schema\n",
    "raw_df.printSchema()\n",
    "\n",
    "# Write to Delta table\n",
    "(raw_df.writeStream\n",
    "    .format(\"delta\")\n",
    "    .option(\"checkpointLocation\", f\"{volume_path}/checkpoints/raw_sensor_data\")\n",
    "    .option(\"mergeSchema\", \"true\")\n",
    "    .outputMode(\"append\")\n",
    "    .trigger(once=True)\n",
    "    .table(\"arao.aerodemo.raw_sensor_data\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aeb7b15f-85a8-4e9d-aa54-4a9fab047a5c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "# Define the schema explicitly\n",
    "maintenance_schema = StructType([\n",
    "    StructField(\"aircraft_id\", StringType(), True),\n",
    "    StructField(\"event_date\", DateType(), True),\n",
    "    StructField(\"event_type\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Use a dedicated path for maintenance events\n",
    "volume_path = \"/Volumes/arao/aerodemo/tmp/maintenance\"\n",
    "\n",
    "# Define Auto Loader stream\n",
    "events_df = (spark.readStream\n",
    "    .format(\"cloudFiles\")\n",
    "    .option(\"cloudFiles.format\", \"csv\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"pathGlobFilter\", \"*.csv\")  # Only load CSVs\n",
    "    .option(\"cloudFiles.schemaLocation\", f\"{volume_path}/schema/maintenance_events\")  # Track schema\n",
    "    .schema(maintenance_schema)\n",
    "    .load(volume_path))\n",
    "\n",
    "# Write to Delta table\n",
    "(events_df.writeStream\n",
    "    .format(\"delta\")\n",
    "    .option(\"checkpointLocation\", f\"{volume_path}/checkpoints/maintenance_events\")\n",
    "    .option(\"mergeSchema\", \"true\")\n",
    "    .outputMode(\"append\")\n",
    "    .trigger(once=True)\n",
    "    .table(\"arao.aerodemo.maintenance_events\"))"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "03A_Autoloader_Load",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
