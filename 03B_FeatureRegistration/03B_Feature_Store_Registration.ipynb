{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e65706f4-8ace-4a01-bcb6-9396430eacc3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# üìò Feature Store Registration\n",
    "This notebook registers the engineered sensor features table from the Delta Live Tables (DLT) pipeline\n",
    "into the Databricks Feature Store for easier discoverability, governance, and reuse during model training and inference.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "99ef2322-e30b-4b5d-94f6-1ef5ca86a760",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# df = spark.read.table(\"arao.aerodemo.sensor_features\").cache()\n",
    "# df.count()  # Force materialization to avoid lazy read error\n",
    "# df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"arao.aerodemo.sensor_features_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "20d2316e-6446-4489-9ff5-ed7cefe80f1b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# %sql\n",
    "# ALTER TABLE arao.aerodemo.sensor_features_table \n",
    "# ALTER COLUMN aircraft_id SET NOT NULL;\n",
    "\n",
    "# ALTER TABLE arao.aerodemo.sensor_features_table \n",
    "# ALTER COLUMN timestamp SET NOT NULL;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "538bf290-962b-4eb6-9b97-4bfb4dc5c51c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# %sql\n",
    "# ALTER TABLE arao.aerodemo.sensor_features_table \n",
    "# ADD CONSTRAINT sensor_features_pk \n",
    "# PRIMARY KEY (aircraft_id, timestamp);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c5f8457b-4791-482b-966c-7260fda04a74",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# %sql\n",
    "# ALTER TABLE arao.aerodemo.sensor_features\n",
    "# ADD CONSTRAINT sensor_features_pk PRIMARY KEY (aircraft_id, timestamp);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1d561261-5fcc-4d14-8625-1c3208d48464",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# üßπ 1. Drop the old table if it exists to avoid constraint conflicts\n",
    "spark.sql(\"DROP TABLE IF EXISTS arao.aerodemo.sensor_features_table\")\n",
    "\n",
    "# üì• 2. Read from existing DLT materialized table\n",
    "df_raw = spark.table(\"arao.aerodemo.sensor_features\") \\\n",
    "    .filter(\"aircraft_id IS NOT NULL AND timestamp IS NOT NULL\")\n",
    "\n",
    "# üßº 3. Clean nulls (as double safety) and cast PKs\n",
    "df_clean = df_raw.withColumn(\"aircraft_id\", F.col(\"aircraft_id\").cast(\"string\")) \\\n",
    "                 .withColumn(\"timestamp\", F.col(\"timestamp\").cast(\"string\"))\n",
    "\n",
    "# üíæ 4. Save as Delta table with schema overwrite\n",
    "df_clean.write.format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"overwriteSchema\", \"true\") \\\n",
    "    .saveAsTable(\"arao.aerodemo.sensor_features_table\")\n",
    "\n",
    "# üîê 5. Enforce NOT NULL constraints\n",
    "spark.sql(\"\"\"\n",
    "  ALTER TABLE arao.aerodemo.sensor_features_table \n",
    "  ALTER COLUMN aircraft_id SET NOT NULL\n",
    "\"\"\")\n",
    "spark.sql(\"\"\"\n",
    "  ALTER TABLE arao.aerodemo.sensor_features_table \n",
    "  ALTER COLUMN timestamp SET NOT NULL\n",
    "\"\"\")\n",
    "\n",
    "# üõ°Ô∏è 6. Add primary key constraint (required for Feature Store)\n",
    "spark.sql(\"\"\"\n",
    "  ALTER TABLE arao.aerodemo.sensor_features_table \n",
    "  ADD CONSTRAINT sensor_features_pk \n",
    "  PRIMARY KEY (aircraft_id, timestamp)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7da20af2-fbc3-4517-b227-3ee890d3d5d5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from databricks.feature_store import FeatureStoreClient\n",
    "\n",
    "fs = FeatureStoreClient()\n",
    "\n",
    "fs.create_table(\n",
    "    name=\"arao.aerodemo.sensor_features_table\",\n",
    "    primary_keys=[\"aircraft_id\", \"timestamp\"],\n",
    "    timestamp_keys=[\"timestamp\"],\n",
    "    description=\"Engineered features for anomaly prediction from sensor data\",\n",
    "    df=spark.read.table(\"arao.aerodemo.sensor_features_table\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6e0af5b0-42d0-4641-a98c-abbeae7d3f62",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "# üõ†Ô∏è Feature Store Registration from DLT Materialized Tables\n",
    "\n",
    "### Why do we need these extra steps after defining DLT tables?\n",
    "\n",
    "While Delta Live Tables (`@dlt.table`) provides robust data engineering pipelines, **it does NOT persist primary key constraints** or explicit schema-level constraints like `NOT NULL` or `PRIMARY KEY` in the Delta table metadata.\n",
    "\n",
    "This is because:\n",
    "- DLT focuses on **data flow orchestration and lineage**, not detailed physical table design.\n",
    "- By default, DLT outputs are stored as **materialized views** (even when called `dlt.table`), meaning they don‚Äôt expose all metadata properties directly to downstream systems.\n",
    "- If you want to integrate these tables with systems like the **Databricks Feature Store**, you need:\n",
    "    ‚úÖ A fully materialized, managed Delta table  \n",
    "    ‚úÖ Explicitly defined primary keys (PKs)  \n",
    "    ‚úÖ Cleaned and consistent column types matching Feature Store expectations\n",
    "\n",
    "### Why explicitly set `pipelines.materialize = true`?\n",
    "\n",
    "This ensures:\n",
    "- The table is **materialized on disk** (backed by a Delta table) instead of being purely a logical view.\n",
    "- The DLT system handles physical storage, compaction, and optimization ‚Äî making it queryable like a native table.\n",
    "\n",
    "However, even with `materialize: true`, **DLT does not persist PK constraints** in the Delta metadata.\n",
    "\n",
    "### Why these post-DLT steps?\n",
    "\n",
    "1Ô∏è‚É£ **Drop + Rewrite as Managed Table**  \n",
    "We read the DLT materialized table and write it back as a clean Delta-managed table to ensure:\n",
    "- Schema stability\n",
    "- Proper registration in the metastore\n",
    "\n",
    "2Ô∏è‚É£ **Cast PK Columns**  \n",
    "We cast `aircraft_id` and `event_timestamp` to `string` to match Feature Store‚Äôs strict type requirements.\n",
    "\n",
    "3Ô∏è‚É£ **Apply NOT NULL + PRIMARY KEY Constraints**  \n",
    "Feature Store **requires primary key persistence** at the Delta table level, which DLT does not enforce natively.\n",
    "We add:\n",
    "- `ALTER COLUMN ... SET NOT NULL`  \n",
    "- `ADD CONSTRAINT ... PRIMARY KEY (...)`\n",
    "\n",
    "4Ô∏è‚É£ **Register in Feature Store**  \n",
    "We register the cleaned, constraint-enforced table as a **Feature Store table**, enabling:\n",
    "- Managed feature lookups  \n",
    "- Model training and inference workflows  \n",
    "- Consistent feature governance and tracking\n",
    "\n",
    "---\n",
    "\n",
    "‚úÖ **Summary:**  \n",
    "Even though DLT handles most of the data orchestration, for full Feature Store integration we must:\n",
    "- Ensure physical table materialization  \n",
    "- Enforce schema constraints  \n",
    "- Register tables explicitly\n",
    "\n",
    "This gives us the best of both worlds: DLT‚Äôs orchestration + Feature Store‚Äôs feature governance.\n",
    "\n",
    "\n",
    "### üì¶ Component-Level Feature Store Registration\n",
    "\n",
    "This section registers the engineered feature tables for each aircraft component ‚Äî engine, landing gear, avionics, cabin pressurization, and airframe ‚Äî into the Databricks Feature Store.\n",
    "\n",
    "‚úÖ **What‚Äôs included:**\n",
    "- **component_features_engine**  \n",
    "  Engine-related lag features and rolling averages (e.g., temperature, vibration, oil pressure).\n",
    "  \n",
    "- **component_features_landing_gear**  \n",
    "  Landing gear features such as brake wear, brake temperature, and shock absorber health.\n",
    "\n",
    "- **component_features_avionics**  \n",
    "  Avionics system features like signal integrity, error log counts, and system temperature.\n",
    "\n",
    "- **component_features_cabin_pressurization**  \n",
    "  Cabin pressurization features including cabin pressure, seal integrity, and airflow rates.\n",
    "\n",
    "- **component_features_airframe**  \n",
    "  Airframe structural features covering stress points, fatigue crack growth, and integrity measures.\n",
    "\n",
    "‚úÖ **Why register?**\n",
    "Registering these tables ensures:\n",
    "- Centralized management of feature sets.\n",
    "- Versioning and reproducibility for ML models.\n",
    "- Easy integration into model training, batch scoring, and online inference pipelines.\n",
    "\n",
    "Each table is registered with:\n",
    "- `aircraft_id` as the primary key.\n",
    "- `event_timestamp` as the time index.\n",
    "- A detailed description for downstream consumers.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fad44bdf-7abc-4fb2-8ac4-8591f96b0318",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from databricks.feature_store import FeatureStoreClient\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "fs = FeatureStoreClient()\n",
    "\n",
    "# üîß Define table names\n",
    "dlt_table = \"arao.aerodemo.component_features_engine\"\n",
    "managed_table = \"arao.aerodemo.component_features_engine_table\"\n",
    "primary_keys = [\"aircraft_id\", \"event_timestamp\"]\n",
    "\n",
    "# 1Ô∏è‚É£ Drop old managed table if exists\n",
    "spark.sql(f\"DROP TABLE IF EXISTS {managed_table}\")\n",
    "\n",
    "# 2Ô∏è‚É£ Read from DLT materialized table\n",
    "df_raw = spark.read.table(dlt_table).filter(\"aircraft_id IS NOT NULL AND event_timestamp IS NOT NULL\")\n",
    "\n",
    "# 3Ô∏è‚É£ Cast PK columns to string (safe type)\n",
    "df_clean = df_raw.withColumn(\"aircraft_id\", F.col(\"aircraft_id\").cast(\"string\")) \\\n",
    "                 .withColumn(\"event_timestamp\", F.col(\"event_timestamp\").cast(\"string\"))\n",
    "\n",
    "# 4Ô∏è‚É£ Save as managed Delta table (overwrite + overwrite schema)\n",
    "df_clean.write.format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"overwriteSchema\", \"true\") \\\n",
    "    .saveAsTable(managed_table)\n",
    "\n",
    "# 5Ô∏è‚É£ Apply NOT NULL constraints\n",
    "spark.sql(f\"ALTER TABLE {managed_table} ALTER COLUMN aircraft_id SET NOT NULL\")\n",
    "spark.sql(f\"ALTER TABLE {managed_table} ALTER COLUMN event_timestamp SET NOT NULL\")\n",
    "\n",
    "# 6Ô∏è‚É£ Add PRIMARY KEY constraint (required by Feature Store)\n",
    "spark.sql(f\"\"\"\n",
    "    ALTER TABLE {managed_table} \n",
    "    ADD CONSTRAINT component_features_engine_pk \n",
    "    PRIMARY KEY (aircraft_id, event_timestamp)\n",
    "\"\"\")\n",
    "\n",
    "# 7Ô∏è‚É£ Register in Feature Store\n",
    "fs.create_table(\n",
    "    name=managed_table,\n",
    "    primary_keys=primary_keys,\n",
    "    timestamp_keys=[\"event_timestamp\"],\n",
    "    description=\"Engineered features for engine components\",\n",
    "    df=spark.read.table(managed_table)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bdbe3ec7-2ac3-42d7-ace5-f8bc3e72094a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from databricks.feature_store import FeatureStoreClient\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "fs = FeatureStoreClient()\n",
    "\n",
    "# üîß Define table names\n",
    "dlt_table = \"arao.aerodemo.component_features_landing_gear\"\n",
    "managed_table = \"arao.aerodemo.component_features_landing_gear_table\"\n",
    "primary_keys = [\"aircraft_id\", \"event_timestamp\"]\n",
    "\n",
    "# 1Ô∏è‚É£ Drop old managed table if exists\n",
    "spark.sql(f\"DROP TABLE IF EXISTS {managed_table}\")\n",
    "\n",
    "# 2Ô∏è‚É£ Read from DLT materialized table\n",
    "df_raw = spark.read.table(dlt_table).filter(\"aircraft_id IS NOT NULL AND event_timestamp IS NOT NULL\")\n",
    "\n",
    "# 3Ô∏è‚É£ Cast PK columns to string (safe type)\n",
    "df_clean = df_raw.withColumn(\"aircraft_id\", F.col(\"aircraft_id\").cast(\"string\")) \\\n",
    "                 .withColumn(\"event_timestamp\", F.col(\"event_timestamp\").cast(\"string\"))\n",
    "\n",
    "# 4Ô∏è‚É£ Save as managed Delta table (overwrite + overwrite schema)\n",
    "df_clean.write.format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"overwriteSchema\", \"true\") \\\n",
    "    .saveAsTable(managed_table)\n",
    "\n",
    "# 5Ô∏è‚É£ Apply NOT NULL constraints\n",
    "spark.sql(f\"ALTER TABLE {managed_table} ALTER COLUMN aircraft_id SET NOT NULL\")\n",
    "spark.sql(f\"ALTER TABLE {managed_table} ALTER COLUMN event_timestamp SET NOT NULL\")\n",
    "\n",
    "# 6Ô∏è‚É£ Add PRIMARY KEY constraint (required by Feature Store)\n",
    "spark.sql(f\"\"\"\n",
    "    ALTER TABLE {managed_table} \n",
    "    ADD CONSTRAINT component_features_landing_gear_pk \n",
    "    PRIMARY KEY (aircraft_id, event_timestamp)\n",
    "\"\"\")\n",
    "\n",
    "# 7Ô∏è‚É£ Register in Feature Store\n",
    "fs.create_table(\n",
    "    name=managed_table,\n",
    "    primary_keys=primary_keys,\n",
    "    timestamp_keys=[\"event_timestamp\"],\n",
    "    description=\"Engineered features for landing gear components\",\n",
    "    df=spark.read.table(managed_table)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f558ba6e-0967-4cec-aeb6-70fb44d75b62",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from databricks.feature_store import FeatureStoreClient\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "fs = FeatureStoreClient()\n",
    "\n",
    "# üîß Define table names\n",
    "dlt_table = \"arao.aerodemo.component_features_avionics\"\n",
    "managed_table = \"arao.aerodemo.component_features_avionics_table\"\n",
    "primary_keys = [\"aircraft_id\", \"event_timestamp\"]\n",
    "\n",
    "# 1Ô∏è‚É£ Drop old managed table if exists\n",
    "spark.sql(f\"DROP TABLE IF EXISTS {managed_table}\")\n",
    "\n",
    "# 2Ô∏è‚É£ Read from DLT materialized table\n",
    "df_raw = spark.read.table(dlt_table).filter(\"aircraft_id IS NOT NULL AND event_timestamp IS NOT NULL\")\n",
    "\n",
    "# 3Ô∏è‚É£ Cast PK columns to string (safe type)\n",
    "df_clean = df_raw.withColumn(\"aircraft_id\", F.col(\"aircraft_id\").cast(\"string\")) \\\n",
    "                 .withColumn(\"event_timestamp\", F.col(\"event_timestamp\").cast(\"string\"))\n",
    "\n",
    "# 4Ô∏è‚É£ Save as managed Delta table (overwrite + overwrite schema)\n",
    "df_clean.write.format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"overwriteSchema\", \"true\") \\\n",
    "    .saveAsTable(managed_table)\n",
    "\n",
    "# 5Ô∏è‚É£ Apply NOT NULL constraints\n",
    "spark.sql(f\"ALTER TABLE {managed_table} ALTER COLUMN aircraft_id SET NOT NULL\")\n",
    "spark.sql(f\"ALTER TABLE {managed_table} ALTER COLUMN event_timestamp SET NOT NULL\")\n",
    "\n",
    "# 6Ô∏è‚É£ Add PRIMARY KEY constraint (required by Feature Store)\n",
    "spark.sql(f\"\"\"\n",
    "    ALTER TABLE {managed_table} \n",
    "    ADD CONSTRAINT component_features_avionics_pk \n",
    "    PRIMARY KEY (aircraft_id, event_timestamp)\n",
    "\"\"\")\n",
    "\n",
    "# 7Ô∏è‚É£ Register in Feature Store\n",
    "fs.create_table(\n",
    "    name=managed_table,\n",
    "    primary_keys=primary_keys,\n",
    "    timestamp_keys=[\"event_timestamp\"],\n",
    "    description=\"Engineered features for avionics components\",\n",
    "    df=spark.read.table(managed_table)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "099f4444-5198-4688-ace6-c2eab022ed09",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from databricks.feature_store import FeatureStoreClient\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "fs = FeatureStoreClient()\n",
    "\n",
    "# üîß Define table names\n",
    "dlt_table = \"arao.aerodemo.component_features_cabin_pressurization\"\n",
    "managed_table = \"arao.aerodemo.component_features_cabin_pressurization_table\"\n",
    "primary_keys = [\"aircraft_id\", \"event_timestamp\"]\n",
    "\n",
    "# 1Ô∏è‚É£ Drop old managed table if exists\n",
    "spark.sql(f\"DROP TABLE IF EXISTS {managed_table}\")\n",
    "\n",
    "# 2Ô∏è‚É£ Read from DLT materialized table\n",
    "df_raw = spark.read.table(dlt_table).filter(\"aircraft_id IS NOT NULL AND event_timestamp IS NOT NULL\")\n",
    "\n",
    "# 3Ô∏è‚É£ Cast PK columns to string (safe type)\n",
    "df_clean = df_raw.withColumn(\"aircraft_id\", F.col(\"aircraft_id\").cast(\"string\")) \\\n",
    "                 .withColumn(\"event_timestamp\", F.col(\"event_timestamp\").cast(\"string\"))\n",
    "\n",
    "# 4Ô∏è‚É£ Save as managed Delta table (overwrite + overwrite schema)\n",
    "df_clean.write.format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"overwriteSchema\", \"true\") \\\n",
    "    .saveAsTable(managed_table)\n",
    "\n",
    "# 5Ô∏è‚É£ Apply NOT NULL constraints\n",
    "spark.sql(f\"ALTER TABLE {managed_table} ALTER COLUMN aircraft_id SET NOT NULL\")\n",
    "spark.sql(f\"ALTER TABLE {managed_table} ALTER COLUMN event_timestamp SET NOT NULL\")\n",
    "\n",
    "# 6Ô∏è‚É£ Add PRIMARY KEY constraint (required by Feature Store)\n",
    "spark.sql(f\"\"\"\n",
    "    ALTER TABLE {managed_table} \n",
    "    ADD CONSTRAINT component_features_cabin_pressurization_pk \n",
    "    PRIMARY KEY (aircraft_id, event_timestamp)\n",
    "\"\"\")\n",
    "\n",
    "# 7Ô∏è‚É£ Register in Feature Store\n",
    "fs.create_table(\n",
    "    name=managed_table,\n",
    "    primary_keys=primary_keys,\n",
    "    timestamp_keys=[\"event_timestamp\"],\n",
    "    description=\"Engineered features for cabin pressurization components\",\n",
    "    df=spark.read.table(managed_table)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d5835070-537c-4105-a6a4-2dd85466d369",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from databricks.feature_store import FeatureStoreClient\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "fs = FeatureStoreClient()\n",
    "\n",
    "# üîß Define table names\n",
    "dlt_table = \"arao.aerodemo.component_features_airframe\"\n",
    "managed_table = \"arao.aerodemo.component_features_airframe_table\"\n",
    "primary_keys = [\"aircraft_id\", \"event_timestamp\"]\n",
    "\n",
    "# 1Ô∏è‚É£ Drop old managed table if exists\n",
    "spark.sql(f\"DROP TABLE IF EXISTS {managed_table}\")\n",
    "\n",
    "# 2Ô∏è‚É£ Read from DLT materialized table\n",
    "df_raw = spark.read.table(dlt_table).filter(\"aircraft_id IS NOT NULL AND event_timestamp IS NOT NULL\")\n",
    "\n",
    "# 3Ô∏è‚É£ Cast PK columns to string (safe type)\n",
    "df_clean = df_raw.withColumn(\"aircraft_id\", F.col(\"aircraft_id\").cast(\"string\")) \\\n",
    "                 .withColumn(\"event_timestamp\", F.col(\"event_timestamp\").cast(\"string\"))\n",
    "\n",
    "# 4Ô∏è‚É£ Save as managed Delta table (overwrite + overwrite schema)\n",
    "df_clean.write.format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"overwriteSchema\", \"true\") \\\n",
    "    .saveAsTable(managed_table)\n",
    "\n",
    "# 5Ô∏è‚É£ Apply NOT NULL constraints\n",
    "spark.sql(f\"ALTER TABLE {managed_table} ALTER COLUMN aircraft_id SET NOT NULL\")\n",
    "spark.sql(f\"ALTER TABLE {managed_table} ALTER COLUMN event_timestamp SET NOT NULL\")\n",
    "\n",
    "# 6Ô∏è‚É£ Add PRIMARY KEY constraint (required by Feature Store)\n",
    "spark.sql(f\"\"\"\n",
    "    ALTER TABLE {managed_table} \n",
    "    ADD CONSTRAINT component_features_airframe_pk \n",
    "    PRIMARY KEY (aircraft_id, event_timestamp)\n",
    "\"\"\")\n",
    "\n",
    "# 7Ô∏è‚É£ Register in Feature Store\n",
    "fs.create_table(\n",
    "    name=managed_table,\n",
    "    primary_keys=primary_keys,\n",
    "    timestamp_keys=[\"event_timestamp\"],\n",
    "    description=\"Engineered features for airframe components\",\n",
    "    df=spark.read.table(managed_table)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "470bf763-1ab4-4c7c-b61f-6d5c558507f6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    " ### ‚úÖ Summary of Registered Feature Tables\n",
    "\n",
    "Below is a consolidated overview of all feature tables registered in the Databricks Feature Store for the AeroDemo project, including their primary keys and schema details.\n",
    "\n",
    "---\n",
    "\n",
    "#### üöÄ 1. `arao.aerodemo.component_features_engine_table`\n",
    "\n",
    "**Primary Keys:**  \n",
    "- `aircraft_id`  \n",
    "- `event_timestamp`\n",
    "\n",
    "**Schema:**  \n",
    "- aircraft_id (string)  \n",
    "- component_id (string)  \n",
    "- event_timestamp (string)  \n",
    "- thrust_level (double)  \n",
    "- fuel_consumption_rate (double)  \n",
    "- temperature_reading (double)  \n",
    "- vibration_level (double)  \n",
    "- oil_pressure (double)  \n",
    "- health_status (string)  \n",
    "- prev_temp (double)  \n",
    "- prev_vibration (double)  \n",
    "- avg_temp_7d (double)  \n",
    "- avg_vibration_7d (double)  \n",
    "- avg_oil_pressure_7d (double)\n",
    "\n",
    "---\n",
    "\n",
    "#### üöÄ 2. `arao.aerodemo.component_features_landing_gear_table`\n",
    "\n",
    "**Primary Keys:**  \n",
    "- `aircraft_id`  \n",
    "- `event_timestamp`\n",
    "\n",
    "**Schema:**  \n",
    "- aircraft_id (string)  \n",
    "- component_id (string)  \n",
    "- event_timestamp (string)  \n",
    "- hydraulic_pressure (double)  \n",
    "- strut_compression (double)  \n",
    "- brake_wear (double)  \n",
    "- brake_temperature (double)  \n",
    "- shock_absorber_status (double)  \n",
    "- health_status (string)  \n",
    "- prev_brake_temp (double)  \n",
    "- prev_brake_wear (double)  \n",
    "- avg_brake_temp_7d (double)  \n",
    "- avg_brake_wear_7d (double)\n",
    "\n",
    "---\n",
    "\n",
    "#### üöÄ 3. `arao.aerodemo.component_features_airframe_table`\n",
    "\n",
    "**Primary Keys:**  \n",
    "- `aircraft_id`  \n",
    "- `event_timestamp`\n",
    "\n",
    "**Schema:**  \n",
    "- aircraft_id (string)  \n",
    "- component_id (string)  \n",
    "- event_timestamp (string)  \n",
    "- stress_points (double)  \n",
    "- fatigue_crack_growth (double)  \n",
    "- temperature_fluctuations (double)  \n",
    "- structural_integrity (double)  \n",
    "- health_status (string)  \n",
    "- prev_stress_points (double)  \n",
    "- prev_fatigue (double)  \n",
    "- avg_stress_points_7d (double)  \n",
    "- avg_fatigue_7d (double)\n",
    "\n",
    "---\n",
    "\n",
    "#### üöÄ 4. `arao.aerodemo.component_features_avionics_table`\n",
    "\n",
    "**Primary Keys:**  \n",
    "- `aircraft_id`  \n",
    "- `event_timestamp`\n",
    "\n",
    "**Schema:**  \n",
    "- aircraft_id (string)  \n",
    "- component_id (string)  \n",
    "- event_timestamp (string)  \n",
    "- power_status (double)  \n",
    "- signal_integrity (double)  \n",
    "- data_transmission_rate (double)  \n",
    "- system_temperature (double)  \n",
    "- error_logs (int)  \n",
    "- health_status (string)  \n",
    "- prev_signal_integrity (double)  \n",
    "- prev_error_logs (int)  \n",
    "- avg_signal_integrity_7d (double)  \n",
    "- avg_error_logs_7d (double)\n",
    "\n",
    "---\n",
    "\n",
    "#### üöÄ 5. `arao.aerodemo.component_features_cabin_pressurization_table`\n",
    "\n",
    "**Primary Keys:**  \n",
    "- `aircraft_id`  \n",
    "- `event_timestamp`\n",
    "\n",
    "**Schema:**  \n",
    "- aircraft_id (string)  \n",
    "- component_id (string)  \n",
    "- event_timestamp (string)  \n",
    "- cabin_pressure (double)  \n",
    "- seal_integrity (double)  \n",
    "- airflow_rate (double)  \n",
    "- temperature_control (double)  \n",
    "- humidity_level (double)  \n",
    "- health_status (string)  \n",
    "- prev_cabin_pressure (double)  \n",
    "- prev_seal_integrity (double)  \n",
    "- avg_cabin_pressure_7d (double)  \n",
    "- avg_seal_integrity_7d (double)\n",
    "\n",
    "---\n",
    "\n",
    "#### üöÄ 6. `arao.aerodemo.sensor_features_table`\n",
    "\n",
    "**Primary Keys:**  \n",
    "- `aircraft_id`  \n",
    "- `timestamp`\n",
    "\n",
    "**Schema:**  \n",
    "- timestamp (string)  \n",
    "- aircraft_id (string)  \n",
    "- model (string)  \n",
    "- engine_temp (double)  \n",
    "- fuel_efficiency (double)  \n",
    "- vibration (double)  \n",
    "- altitude (double)  \n",
    "- airspeed (double)  \n",
    "- oil_pressure (double)  \n",
    "- engine_rpm (int)  \n",
    "- battery_voltage (double)  \n",
    "- anomaly_score (int)  \n",
    "- event_type (string)  \n",
    "- avg_engine_temp_7d (double)  \n",
    "- avg_vibration_7d (double)  \n",
    "- avg_rpm_7d (double)  \n",
    "- prev_anomaly (double)  \n",
    "- days_since_maint (int)  \n",
    "- manufacturer (string)  \n",
    "- engine_type (string)  \n",
    "- capacity (int)  \n",
    "- range_km (int)\n",
    "\n",
    "---\n",
    "\n",
    "### üí° Why is this important?\n",
    "\n",
    "Even though Delta Live Tables (`dlt.table`) creates managed tables, it does **not** enforce or persist primary key constraints on the Delta tables.  \n",
    "The Databricks Feature Store **requires**:\n",
    "‚úÖ Explicit primary key definition  \n",
    "‚úÖ Managed Delta tables (outside `LIVE.` namespace)  \n",
    "‚úÖ Registered schemas and metadata  \n",
    "\n",
    "This is why we:  \n",
    "1Ô∏è‚É£ Extract DLT-generated tables  \n",
    "2Ô∏è‚É£ Re-save them as `overwrite` Delta managed tables  \n",
    "3Ô∏è‚É£ Apply NOT NULL and PRIMARY KEY constraints  \n",
    "4Ô∏è‚É£ Register them into the Feature Store  \n",
    "\n",
    "This ensures the Feature Store can manage features, serve them for ML workflows, and enforce consistency."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "03B_Feature_Store_Registration",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
