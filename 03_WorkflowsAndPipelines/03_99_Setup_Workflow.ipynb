{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a7c9b94d-0b26-4e00-80cc-4fa7b79937b1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 🚀 99_Setup_Workflow Notebook\n",
    "\n",
    "%md\n",
    "### 🛠 99_Setup_Workflow.ipynb\n",
    "This notebook creates or replaces a Databricks Workflow for the AeroDemo project.\n",
    "It chains together the 01_ and 02_ series notebooks, using a fresh ML-enabled, autoscaling cluster (1–6 workers) for each task.\n",
    "\n",
    "✅ Uses Databricks Runtime ML (latest: 14.3.x-cpu-ml-scala2.12)\n",
    "✅ Configures autoscale (1–6 workers)\n",
    "✅ Auto-terminates after 60 minutes idle\n",
    "---\n",
    "\n",
    "### 📋 Current notebooks included:\n",
    "<strike>✅ `01_Table_Creation.ipynb`</strike>  \n",
    "✅ `02_01_Sensor_Data_Generation.ipynb`  \n",
    "✅ `02_02_Engine_Data_Generation.ipynb`  \n",
    "✅ `02_03_CabinPressurization_Data_Generation.ipynb`  \n",
    "✅ `02_04_Airframe_Synthetic_Data_Generation.ipynb`  \n",
    "✅ `02_05_LandingGear_Data_Generation.ipynb`  \n",
    "✅ `02_06_Avionics_Data_Generation.ipynb`  \n",
    "✅ `02_07_ElectricalSystems_Data_Generation.ipynb`  \n",
    "✅ `02_08_FuelSystems_Data_Generation.ipynb`  \n",
    "✅ `02_09_HydraulicSystems_Data_Generation.ipynb`  \n",
    "✅ `02_10_EnvironmentalSystems_Data_Generation.ipynb`\n",
    "\n",
    "---\n",
    "\n",
    "### 🔗 Future additions:\n",
    "We’ll expand this workflow to include:\n",
    "- `03_` series (DLT pipelines)\n",
    "- `04_` series (ML models + scoring)\n",
    "- `05_` series (dashboarding + alerts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "85903cef-0d81-41e0-b7e6-efafc3d7fb16",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### ⚙️ Important Setup Notes\n",
    "\n",
    "✅ **Cluster setup**\n",
    "- Replace `<YOUR_CLUSTER_ID>` in the script with:\n",
    "  - Your existing Databricks cluster ID, **or**\n",
    "  - Switch to `new_cluster` configuration if you want the workflow to create its own cluster\n",
    "\n",
    "✅ **Repo + notebook paths**\n",
    "- Make sure all notebook paths align with:\n",
    "  `/Repos/honnuanand/databricks-aerodemo/<NOTEBOOK_NAME>.ipynb`\n",
    "\n",
    "✅ **Databricks SDK**\n",
    "- This script uses the `databricks-sdk` (Python client).\n",
    "- Run it from:\n",
    "  - A Databricks notebook, **or**\n",
    "  - A local Python environment with `databricks-sdk` installed and configured\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "Once you run this, you’ll have a fresh **AeroDemo_DataPipeline** workflow  \n",
    "that orchestrates all current synthetic data generation steps!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0de58bc4-1e95-4f7a-a1f3-ac62ddf1e07b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 🛠 Workflow Setup Helper Notes\n",
    "\n",
    "This script sets up a Databricks Workflow that runs the AeroDemo synthetic data pipeline notebooks.\n",
    "\n",
    "---\n",
    "\n",
    "✅ **Configurable Parameters**\n",
    "- `NOTEBOOK_BASE_PATH` → Set to the full workspace path where your notebooks live.  \n",
    "  Example: `/Workspace/Users/anand.rao@databricks.com/databricks-aerodemo`\n",
    "\n",
    "- `CLUSTER_ID` → Replace with:\n",
    "  - An **existing cluster ID** you want the workflow to run on, **or**\n",
    "  - Replace `existing_cluster_id` with a `new_cluster` configuration block if you want the workflow to create its own cluster\n",
    "\n",
    "- `WORKFLOW_NAME` → Choose a descriptive name for your Databricks Job (Workflow).\n",
    "\n",
    "---\n",
    "\n",
    "✅ **Notebook Path Pattern**\n",
    "- Each task points to:  \n",
    "  `{NOTEBOOK_BASE_PATH}/{NOTEBOOK_NAME}.ipynb`\n",
    "\n",
    "Make sure your notebook filenames in the workspace exactly match those listed in the `notebooks` array.\n",
    "\n",
    "---\n",
    "\n",
    "✅ **Expansion**\n",
    "- You can later add:\n",
    "  - `03_` series (DLT ingestion pipelines)\n",
    "  - `04_` series (ML model training + scoring)\n",
    "  - `05_` series (visualization + alert notebooks)\n",
    "\n",
    "Just extend the `notebooks` list in the code and rerun the script to update the workflow.\n",
    "\n",
    "---\n",
    "\n",
    "✅ **Execution**\n",
    "- Run this script inside a Databricks notebook or from a local Python environment with the `databricks-sdk` properly configured."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ee88bbb3-8e6e-4688-8d3b-dc629414df7e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Get current notebook path and folder\n",
    "notebook_path = dbutils.notebook.entry_point.getDbutils().notebook().getContext().notebookPath().get()\n",
    "folder_path = os.path.dirname(notebook_path)\n",
    "workspace_path = f\"/Workspace{folder_path}\"\n",
    "\n",
    "print(f\"✅ Notebook path: {notebook_path}\")\n",
    "print(f\"✅ Notebook folder (user-level): {folder_path}\")\n",
    "print(f\"✅ Notebook folder (workspace-level): {workspace_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "573b0971-cfca-4e90-815e-38d308ccdb8d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from databricks.sdk import WorkspaceClient\n",
    "from databricks.sdk.service.jobs import Task, NotebookTask, PipelineTask, TaskDependency\n",
    "\n",
    "# Get current notebook context\n",
    "notebook_path = dbutils.notebook.entry_point.getDbutils().notebook().getContext().notebookPath().get()\n",
    "folder_path = os.path.dirname(notebook_path)\n",
    "NOTEBOOK_BASE_PATH = f\"/Workspace{folder_path}/../02_SyntheticData\"\n",
    "\n",
    "print(f\"✅ Using notebook base path: {NOTEBOOK_BASE_PATH}\")\n",
    "\n",
    "# ---------- CONFIG ----------\n",
    "WORKFLOW_NAME = \"AeroDemo_DataPipeline\"\n",
    "MODE = \"DEV\"\n",
    "EXISTING_CLUSTER_ID = \"0527-220936-f3oreeiv\"\n",
    "DLT_PIPELINE_ID = \"a2ccd850-4b28-4f30-9a53-0fd5f5499713\"\n",
    "\n",
    "# Notebooks list\n",
    "notebooks = [\n",
    "    \"02_01_Synthetic_Data_Generation_v2\",\n",
    "    \"02_02_Engine_Data_Generation\",\n",
    "    \"02_03_CabinPressurization_Data_Generation\",\n",
    "    \"02_04_Airframe_Synthetic_Data_Generation\",\n",
    "    \"02_05_LandingGear_Data_Generation\",\n",
    "    \"02_06_Avionics_Data_Generation\",\n",
    "    \"02_07_ElectricalSystems_Data_Generation\",\n",
    "    \"02_08_FuelSystems_Data_Generation\",\n",
    "    \"02_09_HydraulicSystems_Data_Generation\",\n",
    "    \"02_10_EnvironmentalSystems_Data_Generation\"\n",
    "]\n",
    "\n",
    "w = WorkspaceClient()\n",
    "tasks = []\n",
    "\n",
    "# ✅ Add notebook tasks\n",
    "for idx, notebook in enumerate(notebooks):\n",
    "    notebook_path_full = f\"{NOTEBOOK_BASE_PATH}/{notebook}\"\n",
    "    print(f\"📍 Adding notebook task: {notebook_path_full}\")\n",
    "    task_args = {\n",
    "        \"task_key\": notebook,\n",
    "        \"notebook_task\": NotebookTask(notebook_path=notebook_path_full),\n",
    "        \"existing_cluster_id\": EXISTING_CLUSTER_ID if MODE == \"DEV\" else None\n",
    "    }\n",
    "    if idx > 0:\n",
    "        task_args[\"depends_on\"] = [TaskDependency(task_key=notebooks[idx - 1])]\n",
    "    tasks.append(Task(**task_args))\n",
    "\n",
    "# ✅ Add DLT pipeline task (depends on last notebook)\n",
    "dlt_task_key = \"Run_DLT_Pipeline\"\n",
    "dlt_task = Task(\n",
    "    task_key=dlt_task_key,\n",
    "    pipeline_task=PipelineTask(pipeline_id=DLT_PIPELINE_ID),\n",
    "    depends_on=[TaskDependency(task_key=notebooks[-1])]\n",
    ")\n",
    "print(f\"📍 Adding DLT pipeline task: Pipeline ID {DLT_PIPELINE_ID}\")\n",
    "tasks.append(dlt_task)\n",
    "\n",
    "# ✅ Add final summary notebook task (depends on DLT)\n",
    "final_task_key = \"101_Final_Summary_Task\"\n",
    "final_task_path = f\"{NOTEBOOK_BASE_PATH}/{final_task_key}\"\n",
    "final_task = Task(\n",
    "    task_key=final_task_key,\n",
    "    notebook_task=NotebookTask(notebook_path=final_task_path),\n",
    "    existing_cluster_id=EXISTING_CLUSTER_ID if MODE == \"DEV\" else None,\n",
    "    depends_on=[TaskDependency(task_key=dlt_task_key)]\n",
    ")\n",
    "print(f\"📍 Adding final notebook task: {final_task_path}\")\n",
    "tasks.append(final_task)\n",
    "\n",
    "# ✅ Create or update the job\n",
    "job = w.jobs.create(\n",
    "    name=WORKFLOW_NAME,\n",
    "    tasks=tasks\n",
    ")\n",
    "\n",
    "print(f\"✅ Workflow '{WORKFLOW_NAME}' created/updated with Job ID: {job.job_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cb33b375-41fe-4833-b3fc-2016f5a39ff4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import re\n",
    "\n",
    "# ---------- CONFIG ----------\n",
    "DATABRICKS_INSTANCE = \"https://e2-demo-field-eng.cloud.databricks.com\"\n",
    "TOKEN = \"YOUR_PAT\"\n",
    "JOB_ID = \"173822373344591\"\n",
    "# ----------------------------\n",
    "\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {TOKEN}\",\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "\n",
    "# ✅ Step 1: Get the current job definition\n",
    "get_url = f\"{DATABRICKS_INSTANCE}/api/2.1/jobs/get?job_id={JOB_ID}\"\n",
    "response = requests.get(get_url, headers=headers)\n",
    "\n",
    "if response.status_code != 200:\n",
    "    print(f\"❌ Failed to fetch job: {response.text}\")\n",
    "    exit(1)\n",
    "\n",
    "job_data = response.json()\n",
    "print(f\"✅ Fetched job '{job_data['settings']['name']}'\")\n",
    "\n",
    "# ✅ Step 2: Strip '.ipynb' suffixes in notebook tasks\n",
    "for task in job_data['settings']['tasks']:\n",
    "    if 'notebook_task' in task and 'notebook_path' in task['notebook_task']:\n",
    "        original_path = task['notebook_task']['notebook_path']\n",
    "        cleaned_path = re.sub(r\"\\.ipynb$\", \"\", original_path)\n",
    "        if original_path != cleaned_path:\n",
    "            print(f\"🔧 Fixing: {original_path} → {cleaned_path}\")\n",
    "            task['notebook_task']['notebook_path'] = cleaned_path\n",
    "\n",
    "# ✅ Step 3: Re-submit (reset) the job definition\n",
    "reset_url = f\"{DATABRICKS_INSTANCE}/api/2.1/jobs/reset\"\n",
    "payload = {\n",
    "    \"job_id\": JOB_ID,\n",
    "    \"new_settings\": job_data['settings']\n",
    "}\n",
    "\n",
    "reset_response = requests.post(reset_url, headers=headers, data=json.dumps(payload))\n",
    "\n",
    "if reset_response.status_code != 200:\n",
    "    print(f\"❌ Failed to reset job: {reset_response.text}\")\n",
    "else:\n",
    "    print(f\"✅ Job '{job_data['settings']['name']}' successfully patched!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "edd12228-a5f5-46a0-8d94-8a5bf876c406",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "# ---------- CONFIG ----------\n",
    "DATABRICKS_INSTANCE = \"https://e2-demo-field-eng.cloud.databricks.com\"\n",
    "TOKEN = \"YOUR_PAT\"\n",
    "JOB_ID = \"864722071013094\"\n",
    "DLT_PIPELINE_ID = \"a2ccd850-4b28-4f30-9a53-0fd5f5499713\"\n",
    "# ----------------------------\n",
    "\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {TOKEN}\",\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "\n",
    "# ✅ Step 1: Get current job definition\n",
    "get_url = f\"{DATABRICKS_INSTANCE}/api/2.1/jobs/get?job_id={JOB_ID}\"\n",
    "response = requests.get(get_url, headers=headers)\n",
    "if response.status_code != 200:\n",
    "    print(f\"❌ Failed to fetch job: {response.text}\")\n",
    "    raise SystemExit\n",
    "\n",
    "job_data = response.json()\n",
    "print(f\"✅ Fetched job '{job_data['settings']['name']}'\")\n",
    "\n",
    "# ✅ Step 2: Check if DLT task already exists\n",
    "task_keys = [t['task_key'] for t in job_data['settings']['tasks']]\n",
    "if \"Run_DLT_Pipeline\" in task_keys:\n",
    "    print(\"⚠️ DLT task already exists in the workflow. Skipping add.\")\n",
    "else:\n",
    "    # ✅ Step 3: Add DLT task at the end\n",
    "    job_data['settings']['tasks'].append({\n",
    "        \"task_key\": \"Run_DLT_Pipeline\",\n",
    "        \"depends_on\": [{\"task_key\": \"02_10_EnvironmentalSystems_Data_Generation\"}],\n",
    "        \"pipeline_task\": {\n",
    "            \"pipeline_id\": DLT_PIPELINE_ID\n",
    "        }\n",
    "    })\n",
    "    print(\"✅ DLT task added to workflow payload.\")\n",
    "\n",
    "    # ✅ Step 4: Patch the updated workflow\n",
    "    reset_url = f\"{DATABRICKS_INSTANCE}/api/2.1/jobs/reset\"\n",
    "    payload = {\n",
    "        \"job_id\": JOB_ID,\n",
    "        \"new_settings\": job_data['settings']\n",
    "    }\n",
    "\n",
    "    reset_response = requests.post(reset_url, headers=headers, data=json.dumps(payload))\n",
    "    if reset_response.status_code != 200:\n",
    "        print(f\"❌ Failed to patch job: {reset_response.text}\")\n",
    "    else:\n",
    "        print(f\"✅ Job '{job_data['settings']['name']}' successfully updated with DLT task!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "47b82df1-fbd9-4625-a8bc-f611a6b97344",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# ---------- CONFIG ----------\n",
    "DATABRICKS_INSTANCE = \"https://e2-demo-field-eng.cloud.databricks.com\"\n",
    "TOKEN = \"YOUR_PAT\"\n",
    "JOB_ID = \"864722071013094\"\n",
    "# ----------------------------\n",
    "\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {TOKEN}\",\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "\n",
    "# ✅ Fetch the updated job definition\n",
    "get_url = f\"{DATABRICKS_INSTANCE}/api/2.1/jobs/get?job_id={JOB_ID}\"\n",
    "response = requests.get(get_url, headers=headers)\n",
    "if response.status_code != 200:\n",
    "    print(f\"❌ Failed to fetch job: {response.text}\")\n",
    "    raise SystemExit\n",
    "\n",
    "job_data = response.json()\n",
    "print(f\"✅ Job '{job_data['settings']['name']}' has the following tasks:\")\n",
    "for task in job_data['settings']['tasks']:\n",
    "    if 'notebook_task' in task:\n",
    "        print(f\" - Notebook task: {task['task_key']} → {task['notebook_task']['notebook_path']}\")\n",
    "    if 'pipeline_task' in task:\n",
    "        print(f\" - DLT pipeline task: {task['task_key']} → Pipeline ID: {task['pipeline_task']['pipeline_id']}\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "99_Setup_Workflow",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
