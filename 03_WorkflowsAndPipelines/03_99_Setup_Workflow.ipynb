{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0fad4ba3-c6f1-4e6e-866d-9fb932fc1a17",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# Enables autoreload; learn more at https://docs.databricks.com/en/files/workspace-modules.html#autoreload-for-python-modules\n",
    "# To disable autoreload; run %autoreload 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a7c9b94d-0b26-4e00-80cc-4fa7b79937b1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# üöÄ 99_Setup_Workflow Notebook\n",
    "\n",
    "%md\n",
    "### üõ† 99_Setup_Workflow.ipynb\n",
    "This notebook creates or replaces a Databricks Workflow for the AeroDemo project.\n",
    "It chains together the 01_ and 02_ series notebooks, using a fresh ML-enabled, autoscaling cluster (1‚Äì6 workers) for each task.\n",
    "\n",
    "‚úÖ Uses Databricks Runtime ML (latest: 14.3.x-cpu-ml-scala2.12)\n",
    "‚úÖ Configures autoscale (1‚Äì6 workers)\n",
    "‚úÖ Auto-terminates after 60 minutes idle\n",
    "---\n",
    "\n",
    "### üìã Current notebooks included:\n",
    "<strike>‚úÖ `01_Table_Creation.ipynb`</strike>  \n",
    "‚úÖ `02_01_Sensor_Data_Generation.ipynb`  \n",
    "‚úÖ `02_02_Engine_Data_Generation.ipynb`  \n",
    "‚úÖ `02_03_CabinPressurization_Data_Generation.ipynb`  \n",
    "‚úÖ `02_04_Airframe_Synthetic_Data_Generation.ipynb`  \n",
    "‚úÖ `02_05_LandingGear_Data_Generation.ipynb`  \n",
    "‚úÖ `02_06_Avionics_Data_Generation.ipynb`  \n",
    "‚úÖ `02_07_ElectricalSystems_Data_Generation.ipynb`  \n",
    "‚úÖ `02_08_FuelSystems_Data_Generation.ipynb`  \n",
    "‚úÖ `02_09_HydraulicSystems_Data_Generation.ipynb`  \n",
    "‚úÖ `02_10_EnvironmentalSystems_Data_Generation.ipynb`\n",
    "\n",
    "---\n",
    "\n",
    "### üîó Future additions:\n",
    "We‚Äôll expand this workflow to include:\n",
    "- `03_` series (DLT pipelines)\n",
    "- `04_` series (ML models + scoring)\n",
    "- `05_` series (dashboarding + alerts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "85903cef-0d81-41e0-b7e6-efafc3d7fb16",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### ‚öôÔ∏è Important Setup Notes\n",
    "\n",
    "‚úÖ **Cluster setup**\n",
    "- Replace `<YOUR_CLUSTER_ID>` in the script with:\n",
    "  - Your existing Databricks cluster ID, **or**\n",
    "  - Switch to `new_cluster` configuration if you want the workflow to create its own cluster\n",
    "\n",
    "‚úÖ **Repo + notebook paths**\n",
    "- Make sure all notebook paths align with:\n",
    "  `/Repos/honnuanand/databricks-aerodemo/<NOTEBOOK_NAME>.ipynb`\n",
    "\n",
    "‚úÖ **Databricks SDK**\n",
    "- This script uses the `databricks-sdk` (Python client).\n",
    "- Run it from:\n",
    "  - A Databricks notebook, **or**\n",
    "  - A local Python environment with `databricks-sdk` installed and configured\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "Once you run this, you‚Äôll have a fresh **AeroDemo_DataPipeline** workflow  \n",
    "that orchestrates all current synthetic data generation steps!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0de58bc4-1e95-4f7a-a1f3-ac62ddf1e07b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### üõ† Workflow Setup Helper Notes\n",
    "\n",
    "This script sets up a Databricks Workflow that runs the AeroDemo synthetic data pipeline notebooks.\n",
    "\n",
    "---\n",
    "\n",
    "‚úÖ **Configurable Parameters**\n",
    "- `NOTEBOOK_BASE_PATH` ‚Üí Set to the full workspace path where your notebooks live.  \n",
    "  Example: `/Workspace/Users/anand.rao@databricks.com/databricks-aerodemo`\n",
    "\n",
    "- `CLUSTER_ID` ‚Üí Replace with:\n",
    "  - An **existing cluster ID** you want the workflow to run on, **or**\n",
    "  - Replace `existing_cluster_id` with a `new_cluster` configuration block if you want the workflow to create its own cluster\n",
    "\n",
    "- `WORKFLOW_NAME` ‚Üí Choose a descriptive name for your Databricks Job (Workflow).\n",
    "\n",
    "---\n",
    "\n",
    "‚úÖ **Notebook Path Pattern**\n",
    "- Each task points to:  \n",
    "  `{NOTEBOOK_BASE_PATH}/{NOTEBOOK_NAME}.ipynb`\n",
    "\n",
    "Make sure your notebook filenames in the workspace exactly match those listed in the `notebooks` array.\n",
    "\n",
    "---\n",
    "\n",
    "‚úÖ **Expansion**\n",
    "- You can later add:\n",
    "  - `03_` series (DLT ingestion pipelines)\n",
    "  - `04_` series (ML model training + scoring)\n",
    "  - `05_` series (visualization + alert notebooks)\n",
    "\n",
    "Just extend the `notebooks` list in the code and rerun the script to update the workflow.\n",
    "\n",
    "---\n",
    "\n",
    "‚úÖ **Execution**\n",
    "- Run this script inside a Databricks notebook or from a local Python environment with the `databricks-sdk` properly configured."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c246b924-fae6-4737-83c7-c046cb5bc1f4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import importlib\n",
    "from config import aerodemo_config\n",
    "\n",
    "importlib.reload(aerodemo_config)  # Force reload of the module\n",
    "from config.aerodemo_config import get_config  # Import the get_config function\n",
    "\n",
    "# Create a text widget for environment selection\n",
    "dbutils.widgets.dropdown(\"ENV\", \"dev\", [\"dev\", \"staging\", \"prod\"], \"Environment\")\n",
    "env = dbutils.widgets.get(\"ENV\")\n",
    "config = get_config(env)\n",
    "print(f\"Using environment: {env}\")\n",
    "print(f\"Catalog: {config['catalog']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a3302a4b-ef60-4a97-9279-bf96272d406f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from config.aerodemo_config import get_config\n",
    "\n",
    "# Retrieve configuration for the current environment\n",
    "env = dbutils.widgets.get(\"ENV\")\n",
    "config = get_config(env)\n",
    "\n",
    "print(f\"Using environment: {env}\")\n",
    "print(f\"Catalog: {config['catalog']}\")\n",
    "print(f\"Schema: {config['schema']}\")\n",
    "print(f\"Databricks Instance: {config['databricks_instance']}\")\n",
    "print(f\"Personal Access Token: {config['pat_token']}\")\n",
    "print(f\"E2E Workflow Job ID: {config['e2e_workflow_job_id']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ee88bbb3-8e6e-4688-8d3b-dc629414df7e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Get current notebook path and folder\n",
    "notebook_path = dbutils.notebook.entry_point.getDbutils().notebook().getContext().notebookPath().get()\n",
    "folder_path = os.path.dirname(notebook_path)\n",
    "workspace_path = f\"/Workspace{folder_path}\"\n",
    "\n",
    "print(f\"‚úÖ Notebook path: {notebook_path}\")\n",
    "print(f\"‚úÖ Notebook folder (user-level): {folder_path}\")\n",
    "print(f\"‚úÖ Notebook folder (workspace-level): {workspace_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "573b0971-cfca-4e90-815e-38d308ccdb8d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from databricks.sdk import WorkspaceClient\n",
    "from databricks.sdk.service.jobs import Task, NotebookTask, PipelineTask, TaskDependency\n",
    "from config.aerodemo_config import get_config\n",
    "\n",
    "# Get current notebook context\n",
    "notebook_path = dbutils.notebook.entry_point.getDbutils().notebook().getContext().notebookPath().get()\n",
    "folder_path = os.path.dirname(notebook_path)\n",
    "\n",
    "# Dynamically resolve project base paths\n",
    "workspace_base_path = f\"/Workspace{folder_path}\"\n",
    "project_root = \"/\".join(workspace_base_path.split(\"/\")[:-1])\n",
    "\n",
    "synthetic_data_path = f\"{project_root}/02_SyntheticData\"\n",
    "feature_registration_path = f\"{project_root}/03B_FeatureRegistration\"\n",
    "workflows_path = f\"{project_root}/03_WorkflowsAndPipelines\"\n",
    "\n",
    "# Retrieve configuration\n",
    "env = dbutils.widgets.get(\"ENV\")\n",
    "config = get_config(env)\n",
    "\n",
    "print(f\"‚úÖ Using project root: {project_root}\")\n",
    "\n",
    "# ---------- CONFIG ----------\n",
    "WORKFLOW_NAME = config['workflow_configs']['workflow_name']\n",
    "EXISTING_CLUSTER_ID = config['workflow_configs']['existing_cluster_id']\n",
    "DLT_PIPELINE_ID = config['workflow_configs']['dlt_pipeline_id']\n",
    "\n",
    "# List of synthetic notebooks (parallel)\n",
    "synthetic_notebooks = [\n",
    "    \"02_01_Synthetic_Data_Generation_v2\",\n",
    "    \"02_02_Engine_Data_Generation\",\n",
    "    \"02_03_CabinPressurization_Data_Generation\",\n",
    "    \"02_04_Airframe_Synthetic_Data_Generation\",\n",
    "    \"02_05_LandingGear_Data_Generation\",\n",
    "    \"02_06_Avionics_Data_Generation\",\n",
    "    \"02_07_ElectricalSystems_Data_Generation\",\n",
    "    \"02_08_FuelSystems_Data_Generation\",\n",
    "    \"02_09_HydraulicSystems_Data_Generation\",\n",
    "    \"02_10_EnvironmentalSystems_Data_Generation\"\n",
    "]\n",
    "\n",
    "w = WorkspaceClient()\n",
    "tasks = []\n",
    "\n",
    "# ‚úÖ Add synthetic data notebook tasks (no dependencies)\n",
    "for notebook in synthetic_notebooks:\n",
    "    notebook_path_full = f\"{synthetic_data_path}/{notebook}\"\n",
    "    print(f\"üìç Adding notebook task: {notebook_path_full}\")\n",
    "    tasks.append(Task(\n",
    "        task_key=notebook,\n",
    "        notebook_task=NotebookTask(notebook_path=notebook_path_full),\n",
    "        existing_cluster_id=EXISTING_CLUSTER_ID if env == \"dev\" else None\n",
    "    ))\n",
    "\n",
    "# ‚úÖ Add DLT pipeline task (depends on all synthetic tasks)\n",
    "dlt_task_key = \"Run_DLT_Pipeline\"\n",
    "tasks.append(Task(\n",
    "    task_key=dlt_task_key,\n",
    "    pipeline_task=PipelineTask(pipeline_id=DLT_PIPELINE_ID),\n",
    "    depends_on=[TaskDependency(task_key=nb) for nb in synthetic_notebooks]\n",
    "))\n",
    "print(f\"üìç Adding DLT pipeline task: Pipeline ID {DLT_PIPELINE_ID}\")\n",
    "\n",
    "# ‚úÖ Add Feature Store registration notebook (depends on DLT)\n",
    "feature_reg_task_key = \"03B_Feature_Store_Registration\"\n",
    "feature_reg_path = f\"{feature_registration_path}/03B_Feature_Store_Registration\"\n",
    "tasks.append(Task(\n",
    "    task_key=feature_reg_task_key,\n",
    "    notebook_task=NotebookTask(notebook_path=feature_reg_path),\n",
    "    existing_cluster_id=EXISTING_CLUSTER_ID if env == \"dev\" else None,\n",
    "    depends_on=[TaskDependency(task_key=dlt_task_key)]\n",
    "))\n",
    "print(f\"üìç Adding Feature Store registration task: {feature_reg_path}\")\n",
    "\n",
    "# ‚úÖ Add final summary notebook task (depends on feature registration)\n",
    "final_task_key = \"101_Final_Summary_Task\"\n",
    "final_task_path = f\"{workflows_path}/{final_task_key}\"\n",
    "tasks.append(Task(\n",
    "    task_key=final_task_key,\n",
    "    notebook_task=NotebookTask(notebook_path=final_task_path),\n",
    "    existing_cluster_id=EXISTING_CLUSTER_ID if env == \"dev\" else None,\n",
    "    depends_on=[TaskDependency(task_key=feature_reg_task_key)]\n",
    "))\n",
    "print(f\"üìç Adding final summary notebook task: {final_task_path}\")\n",
    "\n",
    "# ‚úÖ Create or update the job\n",
    "job = w.jobs.create(\n",
    "    name=WORKFLOW_NAME,\n",
    "    tasks=tasks\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Workflow '{WORKFLOW_NAME}' created/updated with Job ID: {job.job_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bf0f97d2-7858-40dc-9f7c-62825d469e2f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "job_id = job.job_id\n",
    "print(f\"‚úÖ Workflow created/updated with Job ID: {job_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2cdee274-bb9a-4799-82ec-db17af23e4f6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "print(\"I am debugging  ============\")\n",
    "\n",
    "# Dynamically get the current notebook‚Äôs directory\n",
    "notebook_path = dbutils.notebook.entry_point.getDbutils().notebook().getContext().notebookPath().get()\n",
    "folder_path = os.path.dirname(notebook_path)\n",
    "workspace_base_path = f\"/Workspace{folder_path}\"\n",
    "\n",
    "# Assume config folder is at the project root\n",
    "project_root = \"/\".join(workspace_base_path.split(\"/\")[:-1])  # remove last folder\n",
    "config_file_path = f\"{project_root}/config/aerodemo_config.py\"\n",
    "\n",
    "print(f\"üìÅ Resolved config file path: {config_file_path}\")\n",
    "\n",
    "# LOCAL FILESYSTEM PATH (for Databricks Files API)\n",
    "local_file_path = \"/Workspace/Users/anand.rao@databricks.com/databricks-aerodemo/config/aerodemo_config.py\"\n",
    "\n",
    "if not os.path.isfile(local_file_path):\n",
    "    print(f\"‚ùå File does not exist at: {local_file_path}\")\n",
    "else:\n",
    "    with open(local_file_path, \"r\") as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    if not lines:\n",
    "        print(\"‚ö†Ô∏è File is empty.\")\n",
    "    else:\n",
    "        print(f\"‚úÖ File has {len(lines)} lines. Showing first 10 lines:\")\n",
    "        for idx, line in enumerate(lines[:10]):\n",
    "            print(f\"{idx + 1:03}: {repr(line.strip())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "db96e7b6-754b-4b44-9333-bbcbd060de7b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Databricks FS path to repo\n",
    "workspace_repo_path = \"file:/Workspace/Repos/anand.rao@databricks.com/databricks-aerodemo/config/aerodemo_config.py\"\n",
    "local_temp_path = \"/tmp/aerodemo_config.py\"\n",
    "\n",
    "try:\n",
    "    dbutils.fs.cp(workspace_repo_path, f\"file:{local_temp_path}\", True)\n",
    "    print(f\"‚úÖ Copied to local path: {local_temp_path}\")\n",
    "\n",
    "    with open(local_temp_path, \"r\") as f:\n",
    "        content = f.read()\n",
    "        print(\"‚úÖ File content preview:\")\n",
    "        print(content[:500])  # just preview first 500 chars\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Failed to copy or read file: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "05112646-0761-49ac-9802-ba4d8dcc895d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# import requests\n",
    "# import json\n",
    "# import re\n",
    "# from config.aerodemo_config import get_config\n",
    "# importlib.reload(aerodemo_config)  # Force reload of the module\n",
    "# # Retrieve configuration for the current environment\n",
    "# env = dbutils.widgets.get(\"ENV\")\n",
    "# config = get_config(env)\n",
    "\n",
    "# # ---------- CONFIG ----------\n",
    "# DATABRICKS_INSTANCE = config['databricks_instance']\n",
    "# TOKEN = config['pat_token']\n",
    "# JOB_ID = config['e2e_workflow_job_id']\n",
    "# # ----------------------------\n",
    "# print(f\"‚úÖ Using Databricks instance: {DATABRICKS_INSTANCE}\")\n",
    "# print(f\"‚úÖ Using PAT token: {TOKEN}\")\n",
    "# print(f\"‚úÖ Using job ID: {JOB_ID}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cb33b375-41fe-4833-b3fc-2016f5a39ff4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# headers = {\n",
    "#     \"Authorization\": f\"Bearer {TOKEN}\",\n",
    "#     \"Content-Type\": \"application/json\"\n",
    "# }\n",
    "\n",
    "# # ‚úÖ Step 1: Get the current job definition\n",
    "# get_url = f\"{DATABRICKS_INSTANCE}/api/2.1/jobs/get?job_id={JOB_ID}\"\n",
    "# response = requests.get(get_url, headers=headers)\n",
    "\n",
    "# if response.status_code != 200:\n",
    "#     print(f\"‚ùå Failed to fetch job: {response.text}\")\n",
    "#     exit(1)\n",
    "\n",
    "# job_data = response.json()\n",
    "# print(f\"‚úÖ Fetched job '{job_data['settings']['name']}'\")\n",
    "\n",
    "# # ‚úÖ Step 2: Strip '.ipynb' suffixes in notebook tasks\n",
    "# for task in job_data['settings']['tasks']:\n",
    "#     if 'notebook_task' in task and 'notebook_path' in task['notebook_task']:\n",
    "#         original_path = task['notebook_task']['notebook_path']\n",
    "#         cleaned_path = re.sub(r\"\\.ipynb$\", \"\", original_path)\n",
    "#         if original_path != cleaned_path:\n",
    "#             print(f\"üîß Fixing: {original_path} ‚Üí {cleaned_path}\")\n",
    "#             task['notebook_task']['notebook_path'] = cleaned_path\n",
    "\n",
    "# # ‚úÖ Step 3: Re-submit (reset) the job definition\n",
    "# reset_url = f\"{DATABRICKS_INSTANCE}/api/2.1/jobs/reset\"\n",
    "# payload = {\n",
    "#     \"job_id\": JOB_ID,\n",
    "#     \"new_settings\": job_data['settings']\n",
    "# }\n",
    "\n",
    "# reset_response = requests.post(reset_url, headers=headers, data=json.dumps(payload))\n",
    "\n",
    "# if reset_response.status_code != 200:\n",
    "#     print(f\"‚ùå Failed to reset job: {reset_response.text}\")\n",
    "# else:\n",
    "#     print(f\"‚úÖ Job '{job_data['settings']['name']}' successfully patched!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "edd12228-a5f5-46a0-8d94-8a5bf876c406",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# import requests\n",
    "# import json\n",
    "# from config.aerodemo_config import get_config\n",
    "\n",
    "# # Retrieve configuration for the current environment\n",
    "# env = dbutils.widgets.get(\"ENV\")\n",
    "# config = get_config(env)\n",
    "\n",
    "# # ---------- CONFIG ----------\n",
    "# DATABRICKS_INSTANCE = config['databricks_instance']\n",
    "# TOKEN = config['pat_token']\n",
    "# JOB_ID = config['e2e_workflow_job_id']\n",
    "# DLT_PIPELINE_ID = config['workflow_configs']['dlt_pipeline_id']\n",
    "# # ----------------------------\n",
    "\n",
    "# headers = {\n",
    "#     \"Authorization\": f\"Bearer {TOKEN}\",\n",
    "#     \"Content-Type\": \"application/json\"\n",
    "# }\n",
    "\n",
    "# # ‚úÖ Step 1: Get current job definition\n",
    "# get_url = f\"{DATABRICKS_INSTANCE}/api/2.1/jobs/get?job_id={JOB_ID}\"\n",
    "# response = requests.get(get_url, headers=headers)\n",
    "# if response.status_code != 200:\n",
    "#     print(f\"‚ùå Failed to fetch job: {response.text}\")\n",
    "#     raise SystemExit\n",
    "\n",
    "# job_data = response.json()\n",
    "# print(f\"‚úÖ Fetched job '{job_data['settings']['name']}'\")\n",
    "\n",
    "# # ‚úÖ Step 2: Check if DLT task already exists\n",
    "# task_keys = [t['task_key'] for t in job_data['settings']['tasks']]\n",
    "# if \"Run_DLT_Pipeline\" in task_keys:\n",
    "#     print(\"‚ö†Ô∏è DLT task already exists in the workflow. Skipping add.\")\n",
    "# else:\n",
    "#     # ‚úÖ Step 3: Add DLT task at the end\n",
    "#     job_data['settings']['tasks'].append({\n",
    "#         \"task_key\": \"Run_DLT_Pipeline\",\n",
    "#         \"depends_on\": [{\"task_key\": \"02_10_EnvironmentalSystems_Data_Generation\"}],\n",
    "#         \"pipeline_task\": {\n",
    "#             \"pipeline_id\": DLT_PIPELINE_ID\n",
    "#         }\n",
    "#     })\n",
    "#     print(\"‚úÖ DLT task added to workflow payload.\")\n",
    "\n",
    "#     # ‚úÖ Step 4: Patch the updated workflow\n",
    "#     reset_url = f\"{DATABRICKS_INSTANCE}/api/2.1/jobs/reset\"\n",
    "#     payload = {\n",
    "#         \"job_id\": JOB_ID,\n",
    "#         \"new_settings\": job_data['settings']\n",
    "#     }\n",
    "\n",
    "#     reset_response = requests.post(reset_url, headers=headers, data=json.dumps(payload))\n",
    "#     if reset_response.status_code != 200:\n",
    "#         print(f\"‚ùå Failed to patch job: {reset_response.text}\")\n",
    "#     else:\n",
    "#         print(f\"‚úÖ Job '{job_data['settings']['name']}' successfully updated with DLT task!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "47b82df1-fbd9-4625-a8bc-f611a6b97344",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# import requests\n",
    "# from config.aerodemo_config import get_config\n",
    "\n",
    "# # Retrieve configuration for the current environment\n",
    "# env = dbutils.widgets.get(\"ENV\")\n",
    "# config = get_config(env)\n",
    "\n",
    "# # ---------- CONFIG ----------\n",
    "# DATABRICKS_INSTANCE = config['databricks_instance']\n",
    "# TOKEN = config['pat_token']\n",
    "# JOB_ID = config['e2e_workflow_job_id']\n",
    "# # ----------------------------\n",
    "\n",
    "# headers = {\n",
    "#     \"Authorization\": f\"Bearer {TOKEN}\",\n",
    "#     \"Content-Type\": \"application/json\"\n",
    "# }\n",
    "\n",
    "# # ‚úÖ Fetch the updated job definition\n",
    "# get_url = f\"{DATABRICKS_INSTANCE}/api/2.1/jobs/get?job_id={JOB_ID}\"\n",
    "# response = requests.get(get_url, headers=headers)\n",
    "# if response.status_code != 200:\n",
    "#     print(f\"‚ùå Failed to fetch job: {response.text}\")\n",
    "#     raise SystemExit\n",
    "\n",
    "# job_data = response.json()\n",
    "# print(f\"‚úÖ Job '{job_data['settings']['name']}' has the following tasks:\")\n",
    "# for task in job_data['settings']['tasks']:\n",
    "#     if 'notebook_task' in task:\n",
    "#         print(f\" - Notebook task: {task['task_key']} ‚Üí {task['notebook_task']['notebook_path']}\")\n",
    "#     if 'pipeline_task' in task:\n",
    "#         print(f\" - DLT pipeline task: {task['task_key']} ‚Üí Pipeline ID: {task['pipeline_task']['pipeline_id']}\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "03_99_Setup_Workflow",
   "widgets": {
    "ENV": {
     "currentValue": "dev",
     "nuid": "f20118ae-2e3b-4709-993f-0332a072a2c5",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "dev",
      "label": "Environment",
      "name": "ENV",
      "options": {
       "widgetDisplayType": "Dropdown",
       "choices": [
        "dev",
        "staging",
        "prod"
       ],
       "fixedDomain": true,
       "multiselect": false
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "dropdown",
      "defaultValue": "dev",
      "label": "Environment",
      "name": "ENV",
      "options": {
       "widgetType": "dropdown",
       "autoCreated": null,
       "choices": [
        "dev",
        "staging",
        "prod"
       ]
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
