{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0fad4ba3-c6f1-4e6e-866d-9fb932fc1a17",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# %load_ext autoreload\n",
    "# %autoreload 2\n",
    "# # Enables autoreload; learn more at https://docs.databricks.com/en/files/workspace-modules.html#autoreload-for-python-modules\n",
    "# # To disable autoreload; run %autoreload 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a7c9b94d-0b26-4e00-80cc-4fa7b79937b1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# üöÄ 99_Setup_Workflow Notebook\n",
    "\n",
    "%md\n",
    "### üõ† 99_Setup_Workflow.ipynb\n",
    "This notebook creates or replaces a Databricks Workflow for the AeroDemo project.\n",
    "It chains together the 01_ and 02_ series notebooks, using a fresh ML-enabled, autoscaling cluster (1‚Äì6 workers) for each task.\n",
    "\n",
    "‚úÖ Uses Databricks Runtime ML (latest: 14.3.x-cpu-ml-scala2.12)\n",
    "‚úÖ Configures autoscale (1‚Äì6 workers)\n",
    "‚úÖ Auto-terminates after 60 minutes idle\n",
    "---\n",
    "\n",
    "### üìã Current notebooks included:\n",
    "<strike>‚úÖ `01_Table_Creation.ipynb`</strike>  \n",
    "‚úÖ `02_01_Sensor_Data_Generation.ipynb`  \n",
    "‚úÖ `02_02_Engine_Data_Generation.ipynb`  \n",
    "‚úÖ `02_03_CabinPressurization_Data_Generation.ipynb`  \n",
    "‚úÖ `02_04_Airframe_Synthetic_Data_Generation.ipynb`  \n",
    "‚úÖ `02_05_LandingGear_Data_Generation.ipynb`  \n",
    "‚úÖ `02_06_Avionics_Data_Generation.ipynb`  \n",
    "‚úÖ `02_07_ElectricalSystems_Data_Generation.ipynb`  \n",
    "‚úÖ `02_08_FuelSystems_Data_Generation.ipynb`  \n",
    "‚úÖ `02_09_HydraulicSystems_Data_Generation.ipynb`  \n",
    "‚úÖ `02_10_EnvironmentalSystems_Data_Generation.ipynb`\n",
    "\n",
    "---\n",
    "\n",
    "### üîó Future additions:\n",
    "We‚Äôll expand this workflow to include:\n",
    "- `03_` series (DLT pipelines)\n",
    "- `04_` series (ML models + scoring)\n",
    "- `05_` series (dashboarding + alerts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "85903cef-0d81-41e0-b7e6-efafc3d7fb16",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### ‚öôÔ∏è Important Setup Notes\n",
    "\n",
    "‚úÖ **Cluster setup**\n",
    "- Replace `<YOUR_CLUSTER_ID>` in the script with:\n",
    "  - Your existing Databricks cluster ID, **or**\n",
    "  - Switch to `new_cluster` configuration if you want the workflow to create its own cluster\n",
    "\n",
    "‚úÖ **Repo + notebook paths**\n",
    "- Make sure all notebook paths align with:\n",
    "  `/Repos/honnuanand/databricks-aerodemo/<NOTEBOOK_NAME>.ipynb`\n",
    "\n",
    "‚úÖ **Databricks SDK**\n",
    "- This script uses the `databricks-sdk` (Python client).\n",
    "- Run it from:\n",
    "  - A Databricks notebook, **or**\n",
    "  - A local Python environment with `databricks-sdk` installed and configured\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "Once you run this, you‚Äôll have a fresh **AeroDemo_DataPipeline** workflow  \n",
    "that orchestrates all current synthetic data generation steps!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0de58bc4-1e95-4f7a-a1f3-ac62ddf1e07b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### üõ† Workflow Setup Helper Notes\n",
    "\n",
    "This script sets up a Databricks Workflow that runs the AeroDemo synthetic data pipeline notebooks.\n",
    "\n",
    "---\n",
    "\n",
    "‚úÖ **Configurable Parameters**\n",
    "- `NOTEBOOK_BASE_PATH` ‚Üí Set to the full workspace path where your notebooks live.  \n",
    "  Example: `/Workspace/Users/anand.rao@databricks.com/databricks-aerodemo`\n",
    "\n",
    "- `CLUSTER_ID` ‚Üí Replace with:\n",
    "  - An **existing cluster ID** you want the workflow to run on, **or**\n",
    "  - Replace `existing_cluster_id` with a `new_cluster` configuration block if you want the workflow to create its own cluster\n",
    "\n",
    "- `WORKFLOW_NAME` ‚Üí Choose a descriptive name for your Databricks Job (Workflow).\n",
    "\n",
    "---\n",
    "\n",
    "‚úÖ **Notebook Path Pattern**\n",
    "- Each task points to:  \n",
    "  `{NOTEBOOK_BASE_PATH}/{NOTEBOOK_NAME}.ipynb`\n",
    "\n",
    "Make sure your notebook filenames in the workspace exactly match those listed in the `notebooks` array.\n",
    "\n",
    "---\n",
    "\n",
    "‚úÖ **Expansion**\n",
    "- You can later add:\n",
    "  - `03_` series (DLT ingestion pipelines)\n",
    "  - `04_` series (ML model training + scoring)\n",
    "  - `05_` series (visualization + alert notebooks)\n",
    "\n",
    "Just extend the `notebooks` list in the code and rerun the script to update the workflow.\n",
    "\n",
    "---\n",
    "\n",
    "‚úÖ **Execution**\n",
    "- Run this script inside a Databricks notebook or from a local Python environment with the `databricks-sdk` properly configured."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a3302a4b-ef60-4a97-9279-bf96272d406f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# üì¶ Load environment configs using Delta-backed config store\n",
    "\n",
    "from conf.config_reader import load_env_configs\n",
    "\n",
    "# Set up the environment (uses Databricks widget)\n",
    "dbutils.widgets.dropdown(\"ENV\", \"dev\", [\"dev\", \"staging\", \"prod\"], \"Environment\")\n",
    "env = dbutils.widgets.get(\"ENV\")\n",
    "\n",
    "# Load configs\n",
    "configs = load_env_configs(env)\n",
    "\n",
    "# Pull individual values as needed\n",
    "DATABRICKS_INSTANCE = configs.get(\"databricks_instance\")\n",
    "TOKEN = configs.get(\"pat_token\")\n",
    "JOB_ID = configs.get(\"e2e_workflow_job_id\")\n",
    "CATALOG = configs.get(\"catalog\")\n",
    "SCHEMA = configs.get(\"schema\")\n",
    "\n",
    "# ‚úÖ Log loaded configs\n",
    "print(f\"‚úÖ Using environment: {env}\")\n",
    "print(f\"  ‚Üí Catalog: {CATALOG}\")\n",
    "print(f\"  ‚Üí Schema: {SCHEMA}\")\n",
    "print(f\"  ‚Üí Databricks Instance: {DATABRICKS_INSTANCE}\")\n",
    "print(f\"  ‚Üí PAT Token: {TOKEN}\")\n",
    "print(f\"  ‚Üí Job ID: {JOB_ID}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ee88bbb3-8e6e-4688-8d3b-dc629414df7e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Get current notebook path and folder\n",
    "notebook_path = dbutils.notebook.entry_point.getDbutils().notebook().getContext().notebookPath().get()\n",
    "folder_path = os.path.dirname(notebook_path)\n",
    "workspace_path = f\"/Workspace{folder_path}\"\n",
    "\n",
    "print(f\"‚úÖ Notebook path: {notebook_path}\")\n",
    "print(f\"‚úÖ Notebook folder (user-level): {folder_path}\")\n",
    "print(f\"‚úÖ Notebook folder (workspace-level): {workspace_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "573b0971-cfca-4e90-815e-38d308ccdb8d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from databricks.sdk import WorkspaceClient\n",
    "from databricks.sdk.service.jobs import Task, NotebookTask, PipelineTask, TaskDependency\n",
    "\n",
    "w = WorkspaceClient()\n",
    "\n",
    "NOTEBOOK_BASE_PATH = \"/Workspace/Users/anand.rao@databricks.com/databricks-aerodemo\"\n",
    "synthetic_data_path = f\"{NOTEBOOK_BASE_PATH}/02_SyntheticData\"\n",
    "feature_registration_path = f\"{NOTEBOOK_BASE_PATH}/03B_FeatureRegistration\"\n",
    "model_training_path = f\"{NOTEBOOK_BASE_PATH}/04_ModelTrainingAndInference\"\n",
    "workflows_path = f\"{NOTEBOOK_BASE_PATH}/03_WorkflowsAndPipelines\"\n",
    "\n",
    "env = \"dev\"\n",
    "config = load_env_configs(env)\n",
    "EXISTING_CLUSTER_ID = config[\"workflow_configs.existing_cluster_id\"]\n",
    "DLT_PIPELINE_ID = config[\"workflow_configs.dlt_pipeline_id\"]\n",
    "WORKFLOW_NAME = config[\"workflow_configs.workflow_name\"]\n",
    "\n",
    "tasks = []\n",
    "\n",
    "# Simple synthetic notebook loop without custom task_key\n",
    "synthetic_notebooks = [\n",
    "    \"02_01_Synthetic_Data_Generation_v2\",\n",
    "    \"02_02_Engine_Data_Generation\",\n",
    "    \"02_03_CabinPressurization_Data_Generation\",\n",
    "    \"02_04_Airframe_Synthetic_Data_Generation\",\n",
    "    \"02_05_LandingGear_Data_Generation\",\n",
    "    \"02_06_Avionics_Data_Generation\",\n",
    "    \"02_07_ElectricalSystems_Data_Generation\",\n",
    "    \"02_08_FuelSystems_Data_Generation\",\n",
    "    \"02_09_HydraulicSystems_Data_Generation\",\n",
    "    \"02_10_EnvironmentalSystems_Data_Generation\"\n",
    "]\n",
    "\n",
    "for notebook in synthetic_notebooks:\n",
    "    tasks.append(Task(\n",
    "        task_key=notebook,  # ‚Üê use the notebook name as the task key\n",
    "        notebook_task=NotebookTask(notebook_path=f\"{synthetic_data_path}/{notebook}\"),\n",
    "        existing_cluster_id=EXISTING_CLUSTER_ID\n",
    "    ))\n",
    "\n",
    "# DLT pipeline task\n",
    "dlt_task_key = \"Run_DLT_Pipeline\"\n",
    "tasks.append(Task(\n",
    "    task_key=dlt_task_key,\n",
    "    pipeline_task=PipelineTask(pipeline_id=DLT_PIPELINE_ID),\n",
    "    depends_on=[TaskDependency(task_key=nb) for nb in synthetic_notebooks]\n",
    "))\n",
    "\n",
    "# Feature store registration\n",
    "feature_task_key = \"Feature_Store_Registration\"\n",
    "tasks.append(Task(\n",
    "    task_key=feature_task_key,\n",
    "    notebook_task=NotebookTask(notebook_path=f\"{feature_registration_path}/03B_Feature_Store_Registration\"),\n",
    "    existing_cluster_id=EXISTING_CLUSTER_ID,\n",
    "    depends_on=[TaskDependency(task_key=dlt_task_key)]\n",
    "))\n",
    "\n",
    "# ML training\n",
    "model_task_key = \"Aircraft_Model_Training\"\n",
    "tasks.append(Task(\n",
    "    task_key=model_task_key,\n",
    "    notebook_task=NotebookTask(notebook_path=f\"{model_training_path}/04_Model_Training_And_Registration\"),\n",
    "    existing_cluster_id=EXISTING_CLUSTER_ID,\n",
    "    depends_on=[TaskDependency(task_key=feature_task_key)]\n",
    "))\n",
    "\n",
    "# NEW: ML inferencing\n",
    "inference_task_key = \"Aircraft_Model_Inference\"\n",
    "tasks.append(Task(\n",
    "    task_key=inference_task_key,\n",
    "    notebook_task=NotebookTask(notebook_path=f\"{model_training_path}/05_Model_Inference\"),\n",
    "    existing_cluster_id=EXISTING_CLUSTER_ID,\n",
    "    depends_on=[TaskDependency(task_key=model_task_key)]\n",
    "))\n",
    "\n",
    "# NEW: Alert generation\n",
    "alerts_task_key = \"Aircraft_Inference_To_Alerts\"\n",
    "tasks.append(Task(\n",
    "    task_key=alerts_task_key,\n",
    "    notebook_task=NotebookTask(notebook_path=f\"{model_training_path}/06_Model_Inference_To_Alerts_table\"),\n",
    "    existing_cluster_id=EXISTING_CLUSTER_ID,\n",
    "    depends_on=[TaskDependency(task_key=inference_task_key)]\n",
    "))\n",
    "\n",
    "# Final summary\n",
    "summary_task_key = \"101_Final_Summary_Task\"\n",
    "tasks.append(Task(\n",
    "    task_key=summary_task_key,\n",
    "    notebook_task=NotebookTask(notebook_path=f\"{workflows_path}/{summary_task_key}\"),\n",
    "    existing_cluster_id=EXISTING_CLUSTER_ID,\n",
    "    depends_on=[TaskDependency(task_key=alerts_task_key)]\n",
    "))\n",
    "\n",
    "# Create/update the workflow\n",
    "job = w.jobs.create(\n",
    "    name=WORKFLOW_NAME,\n",
    "    tasks=tasks\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Workflow '{WORKFLOW_NAME}' created/updated with Job ID: {job.job_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bf0f97d2-7858-40dc-9f7c-62825d469e2f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from conf.config_reader import upsert_config\n",
    "\n",
    "# Set variables\n",
    "# env = \"dev\"  # or \"staging\" / \"prod\" ( already set above)\n",
    "new_job_id = job.job_id  # from your workflow creation step\n",
    "\n",
    "print(f\"‚úÖ Workflow created/updated with Job ID: {new_job_id}\")\n",
    "\n",
    "# üîÑ Update in config store\n",
    "upsert_config(env, \"e2e_workflow_job_id\", str(new_job_id))\n",
    "\n",
    "print(f\"‚úÖ e2e_workflow_job_id updated in config store for environment '{env}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cb33b375-41fe-4833-b3fc-2016f5a39ff4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# headers = {\n",
    "#     \"Authorization\": f\"Bearer {TOKEN}\",\n",
    "#     \"Content-Type\": \"application/json\"\n",
    "# }\n",
    "\n",
    "# # ‚úÖ Step 1: Get the current job definition\n",
    "# get_url = f\"{DATABRICKS_INSTANCE}/api/2.1/jobs/get?job_id={JOB_ID}\"\n",
    "# response = requests.get(get_url, headers=headers)\n",
    "\n",
    "# if response.status_code != 200:\n",
    "#     print(f\"‚ùå Failed to fetch job: {response.text}\")\n",
    "#     exit(1)\n",
    "\n",
    "# job_data = response.json()\n",
    "# print(f\"‚úÖ Fetched job '{job_data['settings']['name']}'\")\n",
    "\n",
    "# # ‚úÖ Step 2: Strip '.ipynb' suffixes in notebook tasks\n",
    "# for task in job_data['settings']['tasks']:\n",
    "#     if 'notebook_task' in task and 'notebook_path' in task['notebook_task']:\n",
    "#         original_path = task['notebook_task']['notebook_path']\n",
    "#         cleaned_path = re.sub(r\"\\.ipynb$\", \"\", original_path)\n",
    "#         if original_path != cleaned_path:\n",
    "#             print(f\"üîß Fixing: {original_path} ‚Üí {cleaned_path}\")\n",
    "#             task['notebook_task']['notebook_path'] = cleaned_path\n",
    "\n",
    "# # ‚úÖ Step 3: Re-submit (reset) the job definition\n",
    "# reset_url = f\"{DATABRICKS_INSTANCE}/api/2.1/jobs/reset\"\n",
    "# payload = {\n",
    "#     \"job_id\": JOB_ID,\n",
    "#     \"new_settings\": job_data['settings']\n",
    "# }\n",
    "\n",
    "# reset_response = requests.post(reset_url, headers=headers, data=json.dumps(payload))\n",
    "\n",
    "# if reset_response.status_code != 200:\n",
    "#     print(f\"‚ùå Failed to reset job: {reset_response.text}\")\n",
    "# else:\n",
    "#     print(f\"‚úÖ Job '{job_data['settings']['name']}' successfully patched!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "edd12228-a5f5-46a0-8d94-8a5bf876c406",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# import requests\n",
    "# import json\n",
    "# from config.aerodemo_config import get_config\n",
    "\n",
    "# # Retrieve configuration for the current environment\n",
    "# env = dbutils.widgets.get(\"ENV\")\n",
    "# config = get_config(env)\n",
    "\n",
    "# # ---------- CONFIG ----------\n",
    "# DATABRICKS_INSTANCE = config['databricks_instance']\n",
    "# TOKEN = config['pat_token']\n",
    "# JOB_ID = config['e2e_workflow_job_id']\n",
    "# DLT_PIPELINE_ID = config['workflow_configs']['dlt_pipeline_id']\n",
    "# # ----------------------------\n",
    "\n",
    "# headers = {\n",
    "#     \"Authorization\": f\"Bearer {TOKEN}\",\n",
    "#     \"Content-Type\": \"application/json\"\n",
    "# }\n",
    "\n",
    "# # ‚úÖ Step 1: Get current job definition\n",
    "# get_url = f\"{DATABRICKS_INSTANCE}/api/2.1/jobs/get?job_id={JOB_ID}\"\n",
    "# response = requests.get(get_url, headers=headers)\n",
    "# if response.status_code != 200:\n",
    "#     print(f\"‚ùå Failed to fetch job: {response.text}\")\n",
    "#     raise SystemExit\n",
    "\n",
    "# job_data = response.json()\n",
    "# print(f\"‚úÖ Fetched job '{job_data['settings']['name']}'\")\n",
    "\n",
    "# # ‚úÖ Step 2: Check if DLT task already exists\n",
    "# task_keys = [t['task_key'] for t in job_data['settings']['tasks']]\n",
    "# if \"Run_DLT_Pipeline\" in task_keys:\n",
    "#     print(\"‚ö†Ô∏è DLT task already exists in the workflow. Skipping add.\")\n",
    "# else:\n",
    "#     # ‚úÖ Step 3: Add DLT task at the end\n",
    "#     job_data['settings']['tasks'].append({\n",
    "#         \"task_key\": \"Run_DLT_Pipeline\",\n",
    "#         \"depends_on\": [{\"task_key\": \"02_10_EnvironmentalSystems_Data_Generation\"}],\n",
    "#         \"pipeline_task\": {\n",
    "#             \"pipeline_id\": DLT_PIPELINE_ID\n",
    "#         }\n",
    "#     })\n",
    "#     print(\"‚úÖ DLT task added to workflow payload.\")\n",
    "\n",
    "#     # ‚úÖ Step 4: Patch the updated workflow\n",
    "#     reset_url = f\"{DATABRICKS_INSTANCE}/api/2.1/jobs/reset\"\n",
    "#     payload = {\n",
    "#         \"job_id\": JOB_ID,\n",
    "#         \"new_settings\": job_data['settings']\n",
    "#     }\n",
    "\n",
    "#     reset_response = requests.post(reset_url, headers=headers, data=json.dumps(payload))\n",
    "#     if reset_response.status_code != 200:\n",
    "#         print(f\"‚ùå Failed to patch job: {reset_response.text}\")\n",
    "#     else:\n",
    "#         print(f\"‚úÖ Job '{job_data['settings']['name']}' successfully updated with DLT task!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "47b82df1-fbd9-4625-a8bc-f611a6b97344",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# import requests\n",
    "# from config.aerodemo_config import get_config\n",
    "\n",
    "# # Retrieve configuration for the current environment\n",
    "# env = dbutils.widgets.get(\"ENV\")\n",
    "# config = get_config(env)\n",
    "\n",
    "# # ---------- CONFIG ----------\n",
    "# DATABRICKS_INSTANCE = config['databricks_instance']\n",
    "# TOKEN = config['pat_token']\n",
    "# JOB_ID = config['e2e_workflow_job_id']\n",
    "# # ----------------------------\n",
    "\n",
    "# headers = {\n",
    "#     \"Authorization\": f\"Bearer {TOKEN}\",\n",
    "#     \"Content-Type\": \"application/json\"\n",
    "# }\n",
    "\n",
    "# # ‚úÖ Fetch the updated job definition\n",
    "# get_url = f\"{DATABRICKS_INSTANCE}/api/2.1/jobs/get?job_id={JOB_ID}\"\n",
    "# response = requests.get(get_url, headers=headers)\n",
    "# if response.status_code != 200:\n",
    "#     print(f\"‚ùå Failed to fetch job: {response.text}\")\n",
    "#     raise SystemExit\n",
    "\n",
    "# job_data = response.json()\n",
    "# print(f\"‚úÖ Job '{job_data['settings']['name']}' has the following tasks:\")\n",
    "# for task in job_data['settings']['tasks']:\n",
    "#     if 'notebook_task' in task:\n",
    "#         print(f\" - Notebook task: {task['task_key']} ‚Üí {task['notebook_task']['notebook_path']}\")\n",
    "#     if 'pipeline_task' in task:\n",
    "#         print(f\" - DLT pipeline task: {task['task_key']} ‚Üí Pipeline ID: {task['pipeline_task']['pipeline_id']}\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "03_99_Setup_Workflow",
   "widgets": {
    "ENV": {
     "currentValue": "dev",
     "nuid": "f20118ae-2e3b-4709-993f-0332a072a2c5",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "dev",
      "label": "Environment",
      "name": "ENV",
      "options": {
       "widgetDisplayType": "Dropdown",
       "choices": [
        "dev",
        "staging",
        "prod"
       ],
       "fixedDomain": true,
       "multiselect": false
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "dropdown",
      "defaultValue": "dev",
      "label": "Environment",
      "name": "ENV",
      "options": {
       "widgetType": "dropdown",
       "autoCreated": null,
       "choices": [
        "dev",
        "staging",
        "prod"
       ]
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
