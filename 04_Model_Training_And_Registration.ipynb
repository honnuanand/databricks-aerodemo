{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1d0dde63-3ba4-46dd-bb4b-7579c6410d10",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# ðŸ“˜ 04_Model_Training_And_Registration\n",
    "\n",
    "This notebook performs the following tasks:\n",
    "- Loads the engineered features from the Delta Live Table pipeline\n",
    "- Trains a simple classification model to predict anomaly risk\n",
    "- Registers the trained model with MLflow for downstream inference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "628aed21-cf52-4298-93fa-a3e4715e3ff8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql.functions import col\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import mlflow\n",
    "import mlflow.sklearn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6641fbe8-f56c-44f4-a495-0054040cff8e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from mlflow.models.signature import infer_signature\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "\n",
    "# Load and prepare data\n",
    "feature_df = spark.read.table(\"arao.aerodemo.sensor_features\").toPandas()\n",
    "feature_df = feature_df.fillna({\"prev_anomaly\": 0.0})\n",
    "X = feature_df[[\n",
    "    \"engine_temp\", \"fuel_efficiency\", \"vibration\", \"altitude\", \"airspeed\",\n",
    "    \"oil_pressure\", \"engine_rpm\", \"battery_voltage\", \"prev_anomaly\"\n",
    "]]\n",
    "y = feature_df[\"anomaly_score\"].astype(int)\n",
    "\n",
    "# Split and scale\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Train the model\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Generate signature after training\n",
    "signature = infer_signature(X_train, model.predict(X_train_scaled))\n",
    "\n",
    "# Log with MLflow\n",
    "with mlflow.start_run(run_name=\"Aircraft_Anomaly_RF_Model\"):\n",
    "    preds = model.predict(X_test_scaled)\n",
    "    report = classification_report(y_test, preds, output_dict=True)\n",
    "\n",
    "    mlflow.log_params(model.get_params())\n",
    "    if \"1\" in report:\n",
    "        mlflow.log_metrics({\n",
    "            \"precision\": report[\"1\"].get(\"precision\", 0.0),\n",
    "            \"recall\": report[\"1\"].get(\"recall\", 0.0),\n",
    "            \"f1-score\": report[\"1\"].get(\"f1-score\", 0.0)\n",
    "        })\n",
    "\n",
    "    mlflow.sklearn.log_model(\n",
    "        sk_model=model,\n",
    "        artifact_path=\"model\",\n",
    "        signature=signature,\n",
    "        registered_model_name=\"AircraftAnomalyPredictor\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a1da6453-fd4c-4cad-aca2-2b364f2c88e0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Check for nulls\n",
    "print(\"NaN count per column in X_train:\")\n",
    "print(X_train.isnull().sum())\n",
    "\n",
    "# Check data types\n",
    "print(\"\\nData types:\")\n",
    "print(X_train.dtypes)\n",
    "\n",
    "# Optional: show rows with NaNs\n",
    "print(\"\\nSample rows with NaNs:\")\n",
    "print(X_train[X_train.isnull().any(axis=1)].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b2ee055a-464e-4fff-a598-ba97d1a71f49",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from mlflow.models.signature import infer_signature\n",
    "import pandas as pd\n",
    "\n",
    "# Drop rows with NaNs\n",
    "df_cleaned = df.dropna()\n",
    "\n",
    "# Separate target\n",
    "y = df_cleaned[\"anomaly_score\"]\n",
    "\n",
    "# Drop timestamp and prepare features\n",
    "X = df_cleaned.drop(columns=[\"anomaly_score\", \"timestamp\"])\n",
    "\n",
    "# Encode all categorical columns\n",
    "X_encoded = pd.get_dummies(X, columns=[\"aircraft_id\", \"model\", \"event_type\"], drop_first=True)\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_encoded, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train and log model\n",
    "with mlflow.start_run(run_name=\"Aircraft_Anomaly_RF_Model\"):\n",
    "    model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    preds = model.predict(X_test)\n",
    "    report = classification_report(y_test, preds, output_dict=True)\n",
    "    print(classification_report(y_test, preds))\n",
    "\n",
    "    # Log model with signature\n",
    "    signature = infer_signature(X_train, model.predict(X_train))\n",
    "\n",
    "    mlflow.sklearn.log_model(\n",
    "        model,\n",
    "        artifact_path=\"model\",\n",
    "        registered_model_name=\"AircraftAnomalyPredictor\",\n",
    "        signature=signature\n",
    "    )\n",
    "\n",
    "    mlflow.log_params({\"model_type\": \"RandomForest\", \"n_estimators\": 100})\n",
    "    mlflow.log_metrics({\n",
    "        \"precision\": report.get(\"1\", {}).get(\"precision\", 0.0),\n",
    "        \"recall\": report.get(\"1\", {}).get(\"recall\", 0.0),\n",
    "        \"f1-score\": report.get(\"1\", {}).get(\"f1-score\", 0.0)\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "39083b64-bc16-4e35-8bda-a11bd33ba1a3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create a DataFrame of predictions\n",
    "pred_df = pd.DataFrame({\n",
    "    \"aircraft_id\": X_test.index,  # Or use the correct aircraft_id if available\n",
    "    \"prediction_date\": pd.Timestamp(\"today\").date(),  # Replace with actual date column if available\n",
    "    \"predicted_anomaly\": preds\n",
    "})\n",
    "\n",
    "# Convert to Spark DataFrame\n",
    "pred_sdf = spark.createDataFrame(pred_df)\n",
    "\n",
    "# Save predictions to Delta table\n",
    "pred_sdf.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"arao.aerodemo.anomaly_predictions\")\n",
    "\n",
    "print(\"âœ… Predictions saved to 'anomaly_predictions' Delta table\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "04_Model_Training_And_Registration",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
