{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c912e2d0-54e9-4ea8-b78c-c1f762865efd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### ğŸ”¹ Raw Sensor Data Ingestion\n",
    "\n",
    "This DLT table ingests streaming CSV files of raw aircraft sensor data using Auto Loader.\n",
    "The schema includes:\n",
    "- `timestamp`, `aircraft_id`, `model`\n",
    "- Sensor metrics like `engine_temp`, `fuel_efficiency`, `vibration`, `altitude`, `airspeed`, and `anomaly_score`.\n",
    "\n",
    "Files are read from a managed volume at `/Volumes/arao/aerodemo/tmp/raw`.\n",
    "\n",
    "âœ… **DLT Benefits**:\n",
    "- Built-in schema enforcement\n",
    "- Automatic lineage\n",
    "- Continuous ingestion readiness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e1f919c9-4b41-4f2d-b457-1562a3a01f27",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import dlt\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "@dlt.table(\n",
    "    name=\"raw_sensor_data\",\n",
    "    comment=\"Ingested raw sensor data from aircraft using Auto Loader\"\n",
    ")\n",
    "def load_sensor_data():\n",
    "    return (\n",
    "        spark.readStream\n",
    "        .format(\"cloudFiles\")\n",
    "        .option(\"cloudFiles.format\", \"csv\")\n",
    "        .option(\"header\", \"true\")\n",
    "        .option(\"pathGlobFilter\", \"*.csv\")\n",
    "        .option(\"cloudFiles.schemaLocation\", \"/Volumes/arao/aerodemo/tmp/raw/schema\")\n",
    "        .schema(\"\"\"\n",
    "            timestamp TIMESTAMP,\n",
    "            aircraft_id STRING,\n",
    "            model STRING,\n",
    "            engine_temp DOUBLE,\n",
    "            fuel_efficiency DOUBLE,\n",
    "            vibration DOUBLE,\n",
    "            altitude DOUBLE,\n",
    "            airspeed DOUBLE,\n",
    "            anomaly_score DOUBLE\n",
    "        \"\"\")\n",
    "        .load(\"/Volumes/arao/aerodemo/tmp/raw\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "85992bed-478e-4f95-9d19-e5839d8b2397",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### ğŸ”¹ Maintenance Events Ingestion\n",
    "\n",
    "This DLT table loads structured maintenance event logs for aircraft.\n",
    "Schema includes:\n",
    "- `aircraft_id`, `event_date`, `event_type` (e.g., \"Routine Check\", \"Engine Repair\")\n",
    "\n",
    "Source files are CSVs dropped in `/Volumes/arao/aerodemo/tmp/maintenance`.\n",
    "\n",
    "âœ… **DLT Benefits**:\n",
    "- Data quality enforcement using `@dlt.expect`\n",
    "- Easier governance and visibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cf253b1d-2feb-4bde-abb8-3c651305ed6e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import dlt\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DateType\n",
    "\n",
    "@dlt.table(\n",
    "    comment=\"Ingested maintenance event logs per aircraft\"\n",
    ")\n",
    "@dlt.expect(\"valid_aircraft_id\", \"aircraft_id IS NOT NULL\")\n",
    "@dlt.expect(\"valid_event_type\", \"event_type IN ('Routine Check', 'Engine Repair')\")\n",
    "def maintenance_events():\n",
    "    volume_path = \"/Volumes/arao/aerodemo/tmp/maintenance\"\n",
    "    return (\n",
    "        spark.readStream\n",
    "        .format(\"cloudFiles\")\n",
    "        .option(\"cloudFiles.format\", \"csv\")\n",
    "        .option(\"header\", \"true\")\n",
    "        .option(\"pathGlobFilter\", \"*.csv\")\n",
    "        .option(\"cloudFiles.schemaLocation\", f\"{volume_path}/schema/maintenance_events\")\n",
    "        .load(volume_path)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "641a4ea8-256d-4947-a233-82b543a80895",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### ğŸ”¹ Cleaned Sensor Data\n",
    "\n",
    "This DLT table applies quality filters to the incoming raw sensor data stream to ensure data reliability. The following constraints are enforced:\n",
    "\n",
    "- `engine_temp` must be â‰¤ 700\n",
    "- `fuel_efficiency` must be â‰¥ 50\n",
    "- `vibration` must be â‰¤ 25\n",
    "\n",
    "Additionally, data expectations (`@dlt.expect`) are defined to ensure:\n",
    "- Engine temperature is within 0â€“1000\n",
    "- Fuel efficiency is positive\n",
    "- Vibration is non-negative\n",
    "\n",
    "âœ… **DLT Benefits**:\n",
    "- Streaming data validation\n",
    "- Simplified enforcement of operational thresholds\n",
    "- Automatic schema tracking and lineage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8dec3a11-80cd-47b1-bcd6-2c15a12fe8c0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "@dlt.table(\n",
    "    name=\"cleaned_sensor_data\",\n",
    "    comment=\"Cleaned sensor data after applying quality filters\"\n",
    ")\n",
    "@dlt.expect(\"valid_engine_temp\", \"engine_temp BETWEEN 0 AND 1000\")\n",
    "@dlt.expect(\"valid_fuel_eff\", \"fuel_efficiency > 0\")\n",
    "@dlt.expect(\"valid_vibration\", \"vibration >= 0\")\n",
    "def clean_sensor_data():\n",
    "    return (\n",
    "        dlt.read_stream(\"raw_sensor_data\")\n",
    "        .filter(\n",
    "            (col(\"engine_temp\") <= 700) &\n",
    "            (col(\"fuel_efficiency\") >= 50) &\n",
    "            (col(\"vibration\") <= 25)\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1c6fc416-a6fd-4ce4-8ee9-0bf5e0ee2da8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### ğŸ”¹ Prediction Results\n",
    "\n",
    "This DLT table performs aggregation on cleaned sensor data to produce a daily risk score per aircraft. The logic includes:\n",
    "\n",
    "- Grouping by `aircraft_id` and `timestamp` (converted to date)\n",
    "- Calculating the average `engine_temp` and `vibration`\n",
    "- Applying a formula to compute a `risk_score`:\n",
    "  \n",
    "  \\[\n",
    "  \\text{{risk_score}} = \\left(\\frac{{\\text{{avg(engine_temp)}}}}{700} + \\frac{{\\text{{avg(vibration)}}}}{25}\\right) \\times 50\n",
    "  \\]\n",
    "\n",
    "âœ… **DLT Benefits**:\n",
    "- Continuous scoring as new sensor data arrives\n",
    "- Built-in aggregation and model-based logic\n",
    "- Enables downstream alerting and visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a169b621-c003-434d-963f-9b5e317627e2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "@dlt.table(\n",
    "    name=\"prediction_results\",\n",
    "    comment=\"Predicted AOG risk scores from cleaned sensor data\"\n",
    ")\n",
    "def predict_risk():\n",
    "    df = dlt.read(\"cleaned_sensor_data\")\n",
    "    return (\n",
    "        df.groupBy(\"aircraft_id\", to_date(\"timestamp\").alias(\"prediction_date\"))\n",
    "          .agg(((avg(\"engine_temp\") / 700 + avg(\"vibration\") / 25) * 50).alias(\"risk_score\"))\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "98080776-0fa1-4cfd-871a-8b5ea82ecf6b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "### ğŸ“˜ Enriched Sensor Data with Maintenance Events\n",
    "\n",
    "\n",
    "### ğŸ”¹ Enriched Sensor Data\n",
    "\n",
    "This view joins cleaned sensor records with matching maintenance events on `aircraft_id` and `date(timestamp) = event_date`.\n",
    "\n",
    "Purpose:\n",
    "- Add maintenance context to sensor data\n",
    "- Enable root cause analysis and post-maintenance tracking\n",
    "\n",
    "âœ… **DLT Benefits**:\n",
    "- Built-in lineage from both sensor and maintenance streams\n",
    "- Flexible expansion for ML or analytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8ccf532c-bfd1-475a-a3bf-1440c8227f21",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import dlt\n",
    "from pyspark.sql.functions import col, to_date, max as spark_max\n",
    "from pyspark.sql.window import Window\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "@dlt.table(\n",
    "    comment=\"Sensor data enriched with most recent maintenance event info\"\n",
    ")\n",
    "def enriched_sensor_data():\n",
    "    cleaned_df = dlt.read(\"cleaned_sensor_data\").withColumn(\"reading_date\", to_date(\"timestamp\"))\n",
    "    events_df = dlt.read(\"maintenance_events\").withColumnRenamed(\"event_date\", \"maint_date\")\n",
    "\n",
    "    # Join on aircraft_id, and filter only maintenance events before or on the sensor reading date\n",
    "    joined = cleaned_df.join(events_df, \"aircraft_id\", \"left\") \\\n",
    "        .filter(col(\"maint_date\") <= col(\"reading_date\"))\n",
    "\n",
    "    # Use window function to get the most recent maintenance event before each reading\n",
    "    window = Window.partitionBy(\"aircraft_id\", \"timestamp\").orderBy(col(\"maint_date\").desc())\n",
    "\n",
    "    result = (joined\n",
    "              .withColumn(\"rank\", F.row_number().over(window))\n",
    "              .filter(col(\"rank\") == 1)\n",
    "              .drop(\"rank\", \"reading_date\"))\n",
    "\n",
    "    return result"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "DLT_Pipeline_Full",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
