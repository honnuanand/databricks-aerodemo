{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7a76f67e-bc6c-4c17-bf65-0ceed72716bf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ‚úÖ Utility Notebook: Post-DLT Run Summary Queries\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Get the active Spark session\n",
    "spark = SparkSession.getActiveSession()\n",
    "\n",
    "# 1Ô∏è‚É£ Total component records per aircraft and component type\n",
    "print(\"\\nüîç Total component records per aircraft and component type\")\n",
    "component_summary = spark.sql(\"\"\"\n",
    "SELECT \n",
    "    aircraft_id, \n",
    "    component_type, \n",
    "    COUNT(*) AS record_count\n",
    "FROM arao.aerodemo.digital_twin_component_view\n",
    "GROUP BY aircraft_id, component_type\n",
    "ORDER BY aircraft_id, component_type\n",
    "\"\"\")\n",
    "component_summary.show(truncate=False)\n",
    "\n",
    "# 2Ô∏è‚É£ Alert counts per aircraft and health status\n",
    "print(\"\\nüîç Alert counts per aircraft and health status\")\n",
    "alert_summary = spark.sql(\"\"\"\n",
    "SELECT \n",
    "    aircraft_id, \n",
    "    health_status, \n",
    "    COUNT(*) AS alert_count\n",
    "FROM arao.aerodemo.anomaly_alerts_component\n",
    "GROUP BY aircraft_id, health_status\n",
    "ORDER BY aircraft_id, health_status\n",
    "\"\"\")\n",
    "alert_summary.show(truncate=False)\n",
    "\n",
    "# 3Ô∏è‚É£ Health status over time (daily counts)\n",
    "print(\"\\nüîç Health status over time (daily counts)\")\n",
    "health_over_time = spark.sql(\"\"\"\n",
    "SELECT \n",
    "    TO_DATE(alert_timestamp) AS alert_date,\n",
    "    aircraft_id,\n",
    "    health_status,\n",
    "    COUNT(*) AS count\n",
    "FROM arao.aerodemo.anomaly_alerts_component\n",
    "GROUP BY alert_date, aircraft_id, health_status\n",
    "ORDER BY alert_date, aircraft_id, health_status\n",
    "\"\"\")\n",
    "health_over_time.show(truncate=False)\n",
    "\n",
    "print(\"\\n‚úÖ All utility notebook summary queries completed successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f7bd3b0d-6c96-4c4b-bb68-95ad00ee9f70",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT component_type, COUNT(*) AS null_event_timestamps\n",
    "FROM arao.aerodemo.digital_twin_component_view\n",
    "WHERE event_timestamp IS NULL\n",
    "GROUP BY component_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0db43216-a89b-4cbb-b21b-165cc232aa77",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from inspect import getsource\n",
    "\n",
    "spark = SparkSession.getActiveSession()\n",
    "\n",
    "# ---------- STEP 1: Check raw CSVs ----------\n",
    "print(\"\\nüîç Checking raw CSV files\")\n",
    "\n",
    "sources = {\n",
    "    \"airframe\": \"/Volumes/arao/aerodemo/tmp/airframe\",\n",
    "    \"landing_gear\": \"/Volumes/arao/aerodemo/tmp/landing_gear\",\n",
    "    \"avionics\": \"/Volumes/arao/aerodemo/tmp/avionics\",\n",
    "    \"cabin\": \"/Volumes/arao/aerodemo/tmp/cabin\",\n",
    "    \"engine\": \"/Volumes/arao/aerodemo/tmp/engine\"\n",
    "}\n",
    "\n",
    "for name, path in sources.items():\n",
    "    print(f\"\\n--- {name.upper()} ---\")\n",
    "    df = spark.read.format(\"csv\").option(\"header\", \"true\").load(path)\n",
    "    print(f\"Columns: {df.columns}\")\n",
    "    df.show(3, truncate=False)\n",
    "    df.select(\"event_timestamp\").distinct().show(5, truncate=False)\n",
    "\n",
    "# ---------- STEP 2: Check if .withColumn('event_timestamp') is applied ----------\n",
    "print(\"\\nüîç Checking DLT twin reader functions for event_timestamp parsing\")\n",
    "\n",
    "dlt_functions = [\"twin_airframe\", \"twin_landing_gear\", \"twin_avionics\", \"twin_cabin_pressurization\", \"twin_engine\"]\n",
    "\n",
    "for func_name in dlt_functions:\n",
    "    try:\n",
    "        func = globals()[func_name]\n",
    "        print(f\"\\n--- Checking function: {func_name} ---\")\n",
    "        print(getsource(func))\n",
    "    except KeyError:\n",
    "        print(f\"‚ö†Ô∏è Function {func_name} not defined in this notebook context. Skipping.\")\n",
    "\n",
    "# ---------- STEP 3: Check downstream component_health tables ----------\n",
    "print(\"\\nüîç Checking component_health tables for null event_timestamps\")\n",
    "\n",
    "tables = [\n",
    "    \"component_health_airframe\",\n",
    "    \"component_health_landing_gear\",\n",
    "    \"component_health_avionics\",\n",
    "    \"component_health_cabin_pressurization\",\n",
    "    \"component_health_engine\"\n",
    "]\n",
    "\n",
    "for table in tables:\n",
    "    print(f\"\\n--- {table} ---\")\n",
    "    df = spark.read.table(f\"arao.aerodemo.{table}\")\n",
    "    df.select(\"event_timestamp\").distinct().show(5, truncate=False)\n",
    "    null_count = df.filter(F.col(\"event_timestamp\").isNull()).count()\n",
    "    print(f\"‚ùó Null event_timestamp count: {null_count}\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 2588019255063012,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "Post DLT Utility Cells",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
