{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7a76f67e-bc6c-4c17-bf65-0ceed72716bf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ‚úÖ Utility Notebook: Post-DLT Run Summary Queries\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Get the active Spark session\n",
    "spark = SparkSession.getActiveSession()\n",
    "\n",
    "# 1Ô∏è‚É£ Total component records per aircraft and component type\n",
    "print(\"\\nüîç Total component records per aircraft and component type\")\n",
    "component_summary = spark.sql(\"\"\"\n",
    "SELECT \n",
    "    aircraft_id, \n",
    "    component_type, \n",
    "    COUNT(*) AS record_count\n",
    "FROM arao.aerodemo.digital_twin_component_view\n",
    "GROUP BY aircraft_id, component_type\n",
    "ORDER BY aircraft_id, component_type\n",
    "\"\"\")\n",
    "component_summary.show(truncate=False)\n",
    "\n",
    "# 2Ô∏è‚É£ Alert counts per aircraft and health status\n",
    "print(\"\\nüîç Alert counts per aircraft and health status\")\n",
    "alert_summary = spark.sql(\"\"\"\n",
    "SELECT \n",
    "    aircraft_id, \n",
    "    health_status, \n",
    "    COUNT(*) AS alert_count\n",
    "FROM arao.aerodemo.anomaly_alerts_component\n",
    "GROUP BY aircraft_id, health_status\n",
    "ORDER BY aircraft_id, health_status\n",
    "\"\"\")\n",
    "alert_summary.show(truncate=False)\n",
    "\n",
    "# 3Ô∏è‚É£ Health status over time (daily counts)\n",
    "print(\"\\nüîç Health status over time (daily counts)\")\n",
    "health_over_time = spark.sql(\"\"\"\n",
    "SELECT \n",
    "    TO_DATE(alert_timestamp) AS alert_date,\n",
    "    aircraft_id,\n",
    "    health_status,\n",
    "    COUNT(*) AS count\n",
    "FROM arao.aerodemo.anomaly_alerts_component\n",
    "GROUP BY alert_date, aircraft_id, health_status\n",
    "ORDER BY alert_date, aircraft_id, health_status\n",
    "\"\"\")\n",
    "health_over_time.show(truncate=False)\n",
    "\n",
    "print(\"\\n‚úÖ All utility notebook summary queries completed successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f7bd3b0d-6c96-4c4b-bb68-95ad00ee9f70",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT component_type, COUNT(*) AS null_event_timestamps\n",
    "FROM arao.aerodemo.digital_twin_component_view\n",
    "WHERE event_timestamp IS NULL\n",
    "GROUP BY component_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0db43216-a89b-4cbb-b21b-165cc232aa77",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from inspect import getsource\n",
    "\n",
    "spark = SparkSession.getActiveSession()\n",
    "\n",
    "# ---------- STEP 1: Check raw CSVs ----------\n",
    "print(\"\\nüîç Checking raw CSV files\")\n",
    "\n",
    "sources = {\n",
    "    \"airframe\": \"/Volumes/arao/aerodemo/tmp/airframe\",\n",
    "    \"landing_gear\": \"/Volumes/arao/aerodemo/tmp/landing_gear\",\n",
    "    \"avionics\": \"/Volumes/arao/aerodemo/tmp/avionics\",\n",
    "    \"cabin\": \"/Volumes/arao/aerodemo/tmp/cabin\",\n",
    "    \"engine\": \"/Volumes/arao/aerodemo/tmp/engine\"\n",
    "}\n",
    "\n",
    "for name, path in sources.items():\n",
    "    print(f\"\\n--- {name.upper()} ---\")\n",
    "    df = spark.read.format(\"csv\").option(\"header\", \"true\").load(path)\n",
    "    print(f\"Columns: {df.columns}\")\n",
    "    df.show(3, truncate=False)\n",
    "    df.select(\"event_timestamp\").distinct().show(5, truncate=False)\n",
    "\n",
    "# ---------- STEP 2: Check if .withColumn('event_timestamp') is applied ----------\n",
    "print(\"\\nüîç Checking DLT twin reader functions for event_timestamp parsing\")\n",
    "\n",
    "dlt_functions = [\"twin_airframe\", \"twin_landing_gear\", \"twin_avionics\", \"twin_cabin_pressurization\", \"twin_engine\"]\n",
    "\n",
    "for func_name in dlt_functions:\n",
    "    try:\n",
    "        func = globals()[func_name]\n",
    "        print(f\"\\n--- Checking function: {func_name} ---\")\n",
    "        print(getsource(func))\n",
    "    except KeyError:\n",
    "        print(f\"‚ö†Ô∏è Function {func_name} not defined in this notebook context. Skipping.\")\n",
    "\n",
    "# ---------- STEP 3: Check downstream component_health tables ----------\n",
    "print(\"\\nüîç Checking component_health tables for null event_timestamps\")\n",
    "\n",
    "tables = [\n",
    "    \"component_health_airframe\",\n",
    "    \"component_health_landing_gear\",\n",
    "    \"component_health_avionics\",\n",
    "    \"component_health_cabin_pressurization\",\n",
    "    \"component_health_engine\"\n",
    "]\n",
    "\n",
    "for table in tables:\n",
    "    print(f\"\\n--- {table} ---\")\n",
    "    df = spark.read.table(f\"arao.aerodemo.{table}\")\n",
    "    df.select(\"event_timestamp\").distinct().show(5, truncate=False)\n",
    "    null_count = df.filter(F.col(\"event_timestamp\").isNull()).count()\n",
    "    print(f\"‚ùó Null event_timestamp count: {null_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "709096ad-7c6b-4357-b33a-2d591f9c7f9c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### üõ† DLT Pipeline Update Polling Script\n",
    "\n",
    "This cell uses the Databricks REST API to:\n",
    "\n",
    "‚úÖ Retrieve the status of a specific DLT pipeline update  \n",
    "‚úÖ Print details like:\n",
    "- Update ID\n",
    "- State (e.g., RUNNING, COMPLETED, FAILED)\n",
    "- Cause\n",
    "- Cluster ID\n",
    "- Start / end timestamps\n",
    "- Full JSON response\n",
    "\n",
    "‚úÖ **Optional:** Poll every N seconds until the update reaches a terminal state (COMPLETED, FAILED, or CANCELED)\n",
    "\n",
    "---\n",
    "\n",
    "#### üìã Required Setup:\n",
    "- Set `DATABRICKS_INSTANCE` to your workspace URL\n",
    "- Set `TOKEN` to a Databricks PAT (Personal Access Token)  \n",
    "- Set `DLT_PIPELINE_ID` and `UPDATE_ID` to the pipeline + update you want to track\n",
    "\n",
    "---\n",
    "\n",
    "#### üí° Notes:\n",
    "- You can adjust the `poll_interval_sec` to control how often it checks the status.\n",
    "- To skip polling, just run the first request-and-print part only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "85346862-a019-4a37-9971-1b7724da257c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "# ---------- CONFIG ----------\n",
    "DATABRICKS_INSTANCE = \"https://e2-demo-field-eng.cloud.databricks.com\"\n",
    "TOKEN = \"YOUR_PERSONAL_ACCESS_TOKEN\"\n",
    "DLT_PIPELINE_ID = \"a2ccd850-4b28-4f30-9a53-0fd5f5499713\"\n",
    "UPDATE_ID = \"2f648154-c61b-4cce-b4da-18115dca1064\"\n",
    "poll_interval_sec = 10  # seconds between checks\n",
    "# ----------------------------\n",
    "\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {TOKEN}\",\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "\n",
    "get_update_url = f\"{DATABRICKS_INSTANCE}/api/2.0/pipelines/{DLT_PIPELINE_ID}/updates/{UPDATE_ID}\"\n",
    "\n",
    "while True:\n",
    "    response = requests.get(get_update_url, headers=headers)\n",
    "\n",
    "    if response.status_code != 200:\n",
    "        print(f\"‚ùå Failed to fetch DLT update: {response.text}\")\n",
    "        break\n",
    "\n",
    "    result = response.json()\n",
    "    update = result.get(\"update\", {})\n",
    "    update_id = update.get(\"update_id\")\n",
    "    state = update.get(\"state\")\n",
    "    cause = update.get(\"cause\")\n",
    "    created_at = update.get('creation_time')\n",
    "    readable_time = datetime.fromtimestamp(created_at / 1000).strftime('%Y-%m-%d %H:%M:%S') if created_at else \"N/A\"\n",
    "\n",
    "    print(f\"‚úÖ Update ID: {update_id}\")\n",
    "    print(f\" - State: {state}\")\n",
    "    print(f\" - Cause: {cause}\")\n",
    "    print(f\" - Created at: {readable_time}\")\n",
    "    print(f\" - Cluster ID: {update.get('cluster_id')}\")\n",
    "    print(f\" - Start time: {update.get('start_time')}\")\n",
    "    print(f\" - End time: {update.get('end_time')}\")\n",
    "    print(f\" - Full JSON details:\\n{json.dumps(update, indent=2)}\")\n",
    "\n",
    "    if state in [\"COMPLETED\", \"FAILED\", \"CANCELED\"]:\n",
    "        print(f\"‚ö†Ô∏è Final state reached: {state}\")\n",
    "        break\n",
    "\n",
    "    time.sleep(poll_interval_sec)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 2588019255063012,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "Post DLT Utility Cells",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
