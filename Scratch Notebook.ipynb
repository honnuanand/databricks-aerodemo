{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "edb6978c-2a13-45c3-a6a3-99427d0972cf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "volume_path = \"/Volumes/arao/aerodemo/tmp/raw\"\n",
    "display(dbutils.fs.ls(volume_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a9c6aedb-6fe8-4357-944d-3950727b2001",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.fs.rm(\"/Volumes/arao/aerodemo/tmp/raw/schema\", recurse=True)\n",
    "dbutils.fs.rm(\"/Volumes/arao/aerodemo/tmp/raw/checkpoints\", recurse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a89d5332-deec-4cf2-aaaa-820fbe8d816f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os\n",
    "\n",
    "# Paths to your volume folders\n",
    "raw_path = \"/Volumes/arao/aerodemo/tmp/raw\"\n",
    "maint_path = \"/Volumes/arao/aerodemo/tmp/maintenance\"\n",
    "\n",
    "# Remove all CSV files and schema/checkpoint folders\n",
    "for folder in [raw_path, maint_path]:\n",
    "    dbutils.fs.rm(f\"{folder}/\", True)  # True = recursive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3c438cf6-da57-4ed2-b6a0-e943c579fa6c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "raw_path = \"/Volumes/arao/aerodemo/tmp/raw\"\n",
    "\n",
    "latest_file = sorted(dbutils.fs.ls(raw_path), key=lambda f: f.modificationTime, reverse=True)\n",
    "display(spark.read.format(\"csv\").option(\"header\", True).load(latest_file[0].path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ff137fb6-c7a7-4c80-bdce-c82a2acdf84c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "raw_path = \"/Volumes/arao/aerodemo/tmp/raw\"\n",
    "files = dbutils.fs.ls(raw_path)\n",
    "\n",
    "# Show file names\n",
    "for f in files:\n",
    "    print(f.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bba1a34d-54fc-432a-a860-5736cdf44a19",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Define the expected sensor schema\n",
    "sensor_schema = StructType([\n",
    "    StructField(\"timestamp\", TimestampType(), True),\n",
    "    StructField(\"aircraft_id\", StringType(), True),\n",
    "    StructField(\"model\", StringType(), True),\n",
    "    StructField(\"engine_temp\", DoubleType(), True),\n",
    "    StructField(\"fuel_efficiency\", DoubleType(), True),\n",
    "    StructField(\"vibration\", DoubleType(), True),\n",
    "    StructField(\"altitude\", DoubleType(), True),\n",
    "    StructField(\"airspeed\", DoubleType(), True),\n",
    "    StructField(\"anomaly_score\", DoubleType(), True),\n",
    "    StructField(\"oil_pressure\", DoubleType(), True),\n",
    "    StructField(\"engine_rpm\", IntegerType(), True),\n",
    "    StructField(\"battery_voltage\", DoubleType(), True)\n",
    "])\n",
    "\n",
    "# Read the CSV file using schema\n",
    "df = spark.read \\\n",
    "    .format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .schema(sensor_schema) \\\n",
    "    .load(\"dbfs:/Volumes/arao/aerodemo/tmp/raw/raw_sensor_data_20250516_220905.csv\")\n",
    "\n",
    "df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c79b905a-78b6-401c-aaf0-21d6f5681be5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Clean schema and checkpoint metadata\n",
    "dbutils.fs.rm(\"dbfs:/Volumes/arao/aerodemo/tmp/raw/schema\", True)\n",
    "dbutils.fs.rm(\"dbfs:/Volumes/arao/aerodemo/tmp/raw/checkpoints\", True)\n",
    "dbutils.fs.rm(\"dbfs:/Volumes/arao/aerodemo/tmp/maintenance/schema\", True)\n",
    "dbutils.fs.rm(\"dbfs:/Volumes/arao/aerodemo/tmp/maintenance/checkpoints\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "119c6cbc-2175-4e73-bd82-cb22381093df",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.fs.ls(\"dbfs:/Volumes/arao/aerodemo/tmp/raw/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b1c82516-fa47-4862-8c94-67c1ce3e65b7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.fs.rm(\"dbfs:/Volumes/arao/aerodemo/tmp/raw\", recurse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2de605cf-2d5a-4447-857a-5e3b68c90a00",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.read.table(\"arao.aerodemo.sensor_features\").printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "92c777ac-69be-4312-b1e7-bf1b16e3ce56",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"DROP TABLE IF EXISTS arao.aerodemo.sensor_features_table\")\n",
    "\n",
    "df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"arao.aerodemo.sensor_features_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7b4aa272-3ceb-4520-9b2e-05ddb3167343",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "DESCRIBE TABLE EXTENDED arao.aerodemo.sensor_features;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6813a559-85de-4e63-bc63-365a56dc9ac8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, current_timestamp, lit\n",
    "\n",
    "# Create the full schema from the predictions table\n",
    "predictions = spark.read.table(\"arao.aerodemo.anomaly_predictions\")\n",
    "\n",
    "# Add alert columns (but don’t write yet)\n",
    "from pyspark.sql.functions import current_timestamp, lit\n",
    "from datetime import datetime\n",
    "\n",
    "timestamp_str = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "predictions_with_meta = (\n",
    "    predictions\n",
    "    .withColumn(\"timestamp\", col(\"prediction_date\").cast(\"string\"))\n",
    "    .drop(\"prediction_date\")\n",
    "    .withColumn(\"alert_generated_at\", current_timestamp())\n",
    "    .withColumn(\"batch_id\", lit(timestamp_str))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "53e5c029-1f02-442b-99cd-81e57211dc42",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Drop the old table (⚠️ this removes existing alerts)\n",
    "spark.sql(\"DROP TABLE IF EXISTS arao.aerodemo.anomaly_alerts\")\n",
    "\n",
    "# Recreate it with full schema\n",
    "predictions.limit(0).write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .saveAsTable(\"arao.aerodemo.anomaly_alerts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "384c5ef0-1bcd-4817-a28b-df97cfd7c12d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"SHOW TABLES IN arao.aerodemo\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3261f175-e392-46de-8699-d33eddcd4d38",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Step 1: Create a temp view\n",
    "CREATE OR REPLACE TEMP VIEW updated_alerts AS\n",
    "SELECT *,\n",
    "       date_sub(current_date(), CAST(rand() * 5 AS INT)) AS new_alert_date\n",
    "FROM arao.aerodemo.anomaly_alerts;\n",
    "\n",
    "-- Step 2: Overwrite the table (careful!)\n",
    "CREATE OR REPLACE TABLE arao.aerodemo.anomaly_alerts AS\n",
    "SELECT\n",
    "  aircraft_id,\n",
    "  timestamp,\n",
    "  model,\n",
    "  engine_temp,\n",
    "  fuel_efficiency,\n",
    "  vibration,\n",
    "  altitude,\n",
    "  airspeed,\n",
    "  oil_pressure,\n",
    "  engine_rpm,\n",
    "  battery_voltage,\n",
    "  event_type,\n",
    "  avg_engine_temp_7d,\n",
    "  avg_vibration_7d,\n",
    "  avg_rpm_7d,\n",
    "  prev_anomaly,\n",
    "  days_since_maint,\n",
    "  manufacturer,\n",
    "  engine_type,\n",
    "  capacity,\n",
    "  range_km,\n",
    "  predicted_anomaly,\n",
    "  new_alert_date AS alert_generated_at,\n",
    "  anomaly_score,\n",
    "  batch_id\n",
    "FROM updated_alerts;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0797b971-7c16-4a67-adb8-834dad555d76",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.read.table(\"arao.aerodemo.sensor_features\") \\\n",
    "  .groupBy(\"aircraft_id\", \"timestamp\") \\\n",
    "  .count() \\\n",
    "  .filter(\"count > 1\") \\\n",
    "  .orderBy(\"aircraft_id\", \"timestamp\") \\\n",
    "  .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b3269203-2197-44bb-abf7-7271746c32ab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Use Spark to count records in each DLT table\n",
    "print(\"raw_sensor_data:\", spark.table(\"arao.aerodemo.raw_sensor_data\").count())\n",
    "print(\"cleaned_sensor_data:\", spark.table(\"arao.aerodemo.cleaned_sensor_data\").count())\n",
    "print(\"maintenance_events:\", spark.table(\"arao.aerodemo.maintenance_events\").count())\n",
    "print(\"enriched_sensor_data:\", spark.table(\"arao.aerodemo.enriched_sensor_data\").count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e597a391-2a2a-405c-973b-5d0c93882c1f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dlt_df = spark.table(\"arao.aerodemo.enriched_sensor_data\")\n",
    "\n",
    "dlt_df.groupBy(\"aircraft_id\", \"timestamp\") \\\n",
    "      .count() \\\n",
    "      .filter(\"count > 1\") \\\n",
    "      .orderBy(\"count\", ascending=False) \\\n",
    "      .show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8ff33062-6253-41fc-b106-b41724025049",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "df = spark.table(\"arao.aerodemo.enriched_sensor_data\")\n",
    "\n",
    "df.filter(col(\"aircraft_id\").isNull() | col(\"timestamp\").isNull()).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d1a54bc2-7fc0-4e4d-b740-e91125fb8480",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number\n",
    "\n",
    "pk_window = Window.partitionBy(\"aircraft_id\", \"timestamp\").orderBy(\"timestamp\")\n",
    "deduped_df = df.withColumn(\"row_num\", row_number().over(pk_window)) \\\n",
    "               .filter(\"row_num = 1\") \\\n",
    "               .drop(\"row_num\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f6700c35-634b-4449-b3f6-c79a45dc13d1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import count\n",
    "\n",
    "df = spark.read.table(\"arao.aerodemo.enriched_sensor_data\")\n",
    "df.groupBy(\"aircraft_id\", \"timestamp\") \\\n",
    "  .agg(count(\"*\").alias(\"cnt\")) \\\n",
    "  .filter(\"cnt > 1\") \\\n",
    "  .orderBy(\"cnt\", ascending=False) \\\n",
    "  .show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "26a790af-7564-4faf-95d3-06194239ab37",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.table(\"arao.aerodemo.enriched_sensor_data\")\n",
    "df.select(\"timestamp\").distinct().show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "104e0eb5-46b7-4f5a-a63d-74f2f200b964",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Drop Delta table\n",
    "spark.sql(\"DROP TABLE IF EXISTS arao.aerodemo.raw_sensor_data\")\n",
    "\n",
    "# Drop schema location (clean up previous inferred schema)\n",
    "dbutils.fs.rm(\"dbfs:/Volumes/arao/aerodemo/tmp/raw/schema/raw_sensor_data\", recurse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3f389a11-38e5-432e-af88-7353bf90a0e9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.fs.rm(\"dbfs:/pipelines/arao/aerodemo/_dlt_metadata/checkpoints/cleaned_sensor_data\", recurse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8b423115-129f-4887-b8b2-697183434b9c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"DROP TABLE IF EXISTS arao.aerodemo.cleaned_sensor_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cc00e6ed-9593-473f-b4d4-f3d5a3a15d99",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.read.table(\"arao.aerodemo.raw_sensor_data\").select(\"timestamp\").show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "56f85a40-8469-4186-a165-55084390860d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "df = spark.read.table(\"arao.aerodemo.enriched_sensor_data\")\n",
    "df.groupBy(\"aircraft_id\", \"timestamp\").count().filter(F.col(\"count\") > 1).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "80837323-9c9b-429b-8ebe-69fde9c5b41c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"raw_sensor_data:\", spark.table(\"arao.aerodemo.raw_sensor_data\").count())\n",
    "print(\"cleaned_sensor_data:\", spark.table(\"arao.aerodemo.cleaned_sensor_data\").count())\n",
    "print(\"maintenance_events:\", spark.table(\"arao.aerodemo.maintenance_events\").count())\n",
    "print(\"enriched_sensor_data:\", spark.table(\"arao.aerodemo.enriched_sensor_data\").count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "253a5c6a-0f39-4c8b-9166-0b9d9ff21b3f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = (\n",
    "    spark.read.format(\"csv\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .load(\"/Volumes/arao/aerodemo/tmp/raw\")\n",
    ")\n",
    "\n",
    "df.select(\"timestamp\").show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "24bd869f-3601-4f25-a193-a03b8ce2db33",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "USE CATALOG arao;\n",
    "USE SCHEMA aerodemo;\n",
    "\n",
    "DROP TABLE IF EXISTS raw_sensor_data;\n",
    "DROP TABLE IF EXISTS cleaned_sensor_data;\n",
    "DROP TABLE IF EXISTS maintenance_events;\n",
    "DROP TABLE IF EXISTS sensor_features;\n",
    "DROP TABLE IF EXISTS sensor_features_table;\n",
    "DROP TABLE IF EXISTS prediction_results;\n",
    "DROP TABLE IF EXISTS anomaly_predictions;\n",
    "DROP TABLE IF EXISTS anomaly_alerts;\n",
    "DROP TABLE IF EXISTS anomaly_alerts_sim;\n",
    "\n",
    "DROP MATERIALIZED VIEW IF EXISTS enriched_sensor_data;\n",
    "DROP MATERIALIZED VIEW IF EXISTS aircraft_model_reference_dlt;\n",
    "DROP MATERIALIZED VIEW IF EXISTS aircraft_location_enriched;\n",
    "\n",
    "DROP VIEW IF EXISTS airport_location_reference;\n",
    "DROP VIEW IF EXISTS aircraft_location_reference;\n",
    "DROP VIEW IF EXISTS digital_twin_aircraft_view;\n",
    "DROP VIEW IF EXISTS digital_twin_engine_view;\n",
    "\n",
    "DROP TABLE IF EXISTS labeled_test_examples_d431e3a9;\n",
    "DROP TABLE IF EXISTS unlabeled_examples_d431e3a9;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "794b055c-920b-4246-958e-d182cf0028fa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Drop all relevant tables/views\n",
    "USE CATALOG arao;\n",
    "USE SCHEMA aerodemo;\n",
    "\n",
    "DROP TABLE IF EXISTS raw_sensor_data;\n",
    "DROP TABLE IF EXISTS cleaned_sensor_data;\n",
    "DROP TABLE IF EXISTS maintenance_events;\n",
    "DROP MATERIALIZED VIEW IF EXISTS aircraft_model_reference_dlt;\n",
    "DROP MATERIALIZED VIEW IF EXISTS enriched_sensor_data;\n",
    "DROP TABLE IF EXISTS sensor_features;\n",
    "DROP TABLE IF EXISTS sensor_features_table;\n",
    "DROP TABLE IF EXISTS prediction_results;\n",
    "DROP TABLE IF EXISTS anomaly_predictions;\n",
    "DROP TABLE IF EXISTS anomaly_alerts;\n",
    "DROP TABLE IF EXISTS anomaly_alerts_sim;\n",
    "DROP VIEW IF EXISTS digital_twin_engine_view;\n",
    "DROP VIEW IF EXISTS digital_twin_aircraft_view;\n",
    "DROP VIEW IF EXISTS airport_location_reference;\n",
    "DROP VIEW IF EXISTS aircraft_location_reference;\n",
    "DROP MATERIALIZED VIEW IF EXISTS aircraft_location_enriched;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2dd69f31-7fb6-4b32-b04a-b359150c5d85",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.fs.rm(\"dbfs:/Volumes/arao/aerodemo/tmp/raw/schema/raw_sensor_data\", True)\n",
    "dbutils.fs.rm(\"dbfs:/Volumes/arao/aerodemo/tmp/maintenance/schema\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c8b2a9ae-7074-48cc-9bc6-85643437ee00",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "raw_df = spark.read.option(\"header\", True).csv(\"/Volumes/arao/aerodemo/tmp/raw/\")\n",
    "maint_df = spark.read.option(\"header\", True).csv(\"/Volumes/arao/aerodemo/tmp/maintenance/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "55a70819-91c5-41a4-b5cb-80f8cf6708c4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"✅ Sensor rows:\", raw_df.count())\n",
    "print(\"✅ Maintenance events:\", maint_df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4fedeec9-dad7-49f7-95b8-4e85a460a6a3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "raw_df.select(\"timestamp\").show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ab94da95-e964-491c-b7cc-76f09ddae303",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "raw_df = spark.read.option(\"header\", True).csv(\"/Volumes/arao/aerodemo/tmp/raw/raw_sensor_data_<latest_timestamp>.csv\")\n",
    "raw_df.select(\"timestamp\").show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f1c39a40-ad6d-4a24-bcad-c81bae422ca6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(dbutils.fs.ls(\"/Volumes/arao/aerodemo/tmp/raw/\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2b0312dd-fb76-4f03-ad76-ed5eb1aadb38",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    " latest_file = \"/Volumes/arao/aerodemo/tmp/raw/raw_sensor_data_20250521_171256.csv\"\n",
    "\n",
    "raw_df = spark.read.option(\"header\", True).csv(latest_file)\n",
    "raw_df.select(\"timestamp\").show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9ac9f4de-2fec-48d7-bdc2-97daaaa1ff24",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define paths\n",
    "raw_path = \"/Volumes/arao/aerodemo/tmp/raw\"\n",
    "maint_path = \"/Volumes/arao/aerodemo/tmp/maintenance\"\n",
    "\n",
    "# Keep only this latest file (update if needed)\n",
    "latest_raw = \"raw_sensor_data_20250521_171256.csv\"\n",
    "latest_maint = \"maintenance_events_20250521_171256.csv\"\n",
    "\n",
    "# Clean up raw data files\n",
    "for file in dbutils.fs.ls(raw_path):\n",
    "    if file.name.startswith(\"raw_sensor_data_\") and file.name != latest_raw:\n",
    "        print(f\"Deleting: {file.path}\")\n",
    "        dbutils.fs.rm(file.path)\n",
    "\n",
    "# Clean up maintenance files\n",
    "for file in dbutils.fs.ls(maint_path):\n",
    "    if file.name.startswith(\"maintenance_events_\") and file.name != latest_maint:\n",
    "        print(f\"Deleting: {file.path}\")\n",
    "        dbutils.fs.rm(file.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "072e0fb0-0935-4668-9307-4565ce32f078",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "raw_df = spark.read.option(\"header\", True).csv(\"/Volumes/arao/aerodemo/tmp/raw/\")\n",
    "maint_df = spark.read.option(\"header\", True).csv(\"/Volumes/arao/aerodemo/tmp/maintenance/\")\n",
    "\n",
    "print(\"✅ Sensor records:\", raw_df.count())\n",
    "print(\"✅ Maintenance events:\", maint_df.count())\n",
    "raw_df.select(\"timestamp\").show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7db18138-b499-4f34-9712-82725a9b9a2f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "TRUNCATE TABLE arao.aerodemo.cleaned_sensor_data;\n",
    "TRUNCATE TABLE arao.aerodemo.raw_sensor_data;\n",
    "TRUNCATE TABLE arao.aerodemo.enriched_sensor_data;\n",
    "TRUNCATE TABLE arao.aerodemo.maintenance_events;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "71013795-1a03-4cab-b28b-b1392a628fa4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "DROP TABLE IF EXISTS arao.aerodemo.cleaned_sensor_data;\n",
    "DROP TABLE IF EXISTS arao.aerodemo.raw_sensor_data;\n",
    "DROP TABLE IF EXISTS arao.aerodemo.enriched_sensor_data;\n",
    "DROP TABLE IF EXISTS arao.aerodemo.maintenance_events;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "62e050d6-9f1c-4b08-a547-be77b63aeb24",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Replace with your actual path if different\n",
    "dbutils.fs.rm(\"dbfs:/pipelines/{your_pipeline_id}/_dlt_metadata\", recurse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6601c563-a373-4417-97a8-452d60aec998",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.fs.rm(\"dbfs:/Volumes/arao/aerodemo/tmp/raw/\", recurse=True)\n",
    "dbutils.fs.rm(\"dbfs:/Volumes/arao/aerodemo/tmp/maintenance/\", recurse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "70e217e9-d002-4550-9123-23b9de23c993",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "for table in [\n",
    "    \"raw_sensor_data\", \n",
    "    \"cleaned_sensor_data\", \n",
    "    \"maintenance_events\", \n",
    "    \"aircraft_model_reference\", \n",
    "    \"enriched_sensor_data\", \n",
    "    \"sensor_features\",\n",
    "    \"prediction_results\",\n",
    "    \"digital_twin_engine_view\",\n",
    "    \"digital_twin_aircraft_view\",\n",
    "    \"post_dlt_sanity_check\"\n",
    "]:\n",
    "    print(table, spark.table(f\"arao.aerodemo.{table}\").count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "049817bc-e321-4431-a932-8023d7f8b02a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(dbutils.fs.ls(\"dbfs:/Volumes/arao/aerodemo/tmp/raw\"))\n",
    "display(dbutils.fs.ls(\"dbfs:/Volumes/arao/aerodemo/tmp/maintenance\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "13296810-1465-4532-8daa-e807ce00e04e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "raw_df = spark.read.option(\"header\", True).csv(\"/Volumes/arao/aerodemo/tmp/raw\")\n",
    "raw_df.printSchema()\n",
    "raw_df.show(5, truncate=False)\n",
    "print(\"✅ Sensor rows:\", raw_df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b1cf500d-c9d4-4ba9-b994-897caf940bbf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "maint_df = spark.read.option(\"header\", True).csv(\"/Volumes/arao/aerodemo/tmp/maintenance\")\n",
    "maint_df.printSchema()\n",
    "maint_df.show(5, truncate=False)\n",
    "print(\"✅ Maintenance events:\", maint_df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d0728388-8b66-4e19-8bc1-ac03f2fa187b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, to_timestamp\n",
    "\n",
    "raw_df.select(\n",
    "    col(\"timestamp\"),\n",
    "    to_timestamp(\"timestamp\", \"yyyy-MM-dd HH:mm:ss\").alias(\"parsed_ts\")\n",
    ").show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3648c102-1149-4587-97d0-6edbe1e23e39",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "raw_df.select(\n",
    "    [col(c).isNull().alias(f\"{c}_is_null\") for c in [\"timestamp\", \"aircraft_id\", \"engine_temp\"]]\n",
    ").groupBy().sum().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f7e821c5-5db8-47d4-85c9-b911495cec79",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"DROP TABLE IF EXISTS arao.aerodemo.cleaned_sensor_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ead75531-ad45-4eba-a32b-8417c6b0fc34",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "for table in [\n",
    "    \"raw_sensor_data\", \n",
    "    \"cleaned_sensor_data\", \n",
    "    \"maintenance_events\", \n",
    "    \"aircraft_model_reference\", \n",
    "    \"enriched_sensor_data\", \n",
    "    \"sensor_features\"\n",
    "]:\n",
    "    print(table, spark.table(f\"arao.aerodemo.{table}\").count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e4ddd35a-59bb-4bc2-911b-0574bc4d5d78",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.table(\"arao.aerodemo.raw_sensor_data\")\n",
    "\n",
    "df.select(\"timestamp\", \"aircraft_id\", \"engine_temp\", \"vibration\") \\\n",
    "  .filter(\"engine_temp < 1000 AND vibration >= 0\") \\\n",
    "  .show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d844e144-329a-42a0-a536-183f9ec12757",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import count\n",
    "\n",
    "spark.read.table(\"arao.aerodemo.enriched_sensor_data\") \\\n",
    "    .groupBy(\"aircraft_id\", \"timestamp\") \\\n",
    "    .agg(count(\"*\").alias(\"dup_count\")) \\\n",
    "    .filter(\"dup_count > 1\") \\\n",
    "    .show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b4f45155-0cd2-4696-9354-8e685f43137c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "ALTER TABLE arao.aerodemo.sensor_features\n",
    "ADD CONSTRAINT primaryKey CHECK (aircraft_id IS NOT NULL AND timestamp IS NOT NULL);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e7d7276e-1335-4cd2-9ca1-ac4f087de520",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.createDataFrame(df).write.mode(\"overwrite\").saveAsTable(\"engines\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "57b32d37-0546-4bd1-8fa0-11297f0249b5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.createDataFrame(generate_engines_data()).write.mode(\"overwrite\").saveAsTable(\"arao.aerodemo.engines\")\n",
    "spark.createDataFrame(generate_landing_gear_data()).write.mode(\"overwrite\").saveAsTable(\"arao.aerodemo.landing_gear\")\n",
    "spark.createDataFrame(generate_avionics_data()).write.mode(\"overwrite\").saveAsTable(\"arao.aerodemo.avionics_systems\")\n",
    "spark.createDataFrame(generate_cabin_pressurization_data()).write.mode(\"overwrite\").saveAsTable(\"arao.aerodemo.cabin_pressurization\")\n",
    "spark.createDataFrame(generate_airframe_data()).write.mode(\"overwrite\").saveAsTable(\"arao.aerodemo.airframe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a30e42d5-fc3c-4347-a1ce-a387a5c7f15c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# --- Synthetic Data Generators for Components ---\n",
    "\n",
    "def generate_engines_data(num_records=100):\n",
    "    return pd.DataFrame({\n",
    "        'engine_id': [f'engine_{i}' for i in range(num_records)],\n",
    "        'aircraft_id': [f'A320_101' for _ in range(num_records)],\n",
    "        'thrust_level': np.random.uniform(50000, 120000, num_records),\n",
    "        'fuel_consumption_rate': np.random.uniform(2.0, 5.0, num_records),\n",
    "        'temperature_reading': np.random.uniform(300, 800, num_records),\n",
    "        'vibration_level': np.random.uniform(0.1, 2.0, num_records),\n",
    "        'oil_pressure': np.random.uniform(30, 80, num_records)\n",
    "    })\n",
    "\n",
    "def generate_landing_gear_data(num_records=100):\n",
    "    return pd.DataFrame({\n",
    "        'gear_id': [f'gear_{i}' for i in range(num_records)],\n",
    "        'aircraft_id': [f'A320_101' for _ in range(num_records)],\n",
    "        'hydraulic_pressure': np.random.uniform(2000, 4000, num_records),\n",
    "        'strut_compression': np.random.uniform(5, 15, num_records),\n",
    "        'brake_wear': np.random.uniform(0, 100, num_records),\n",
    "        'brake_temperature': np.random.uniform(100, 500, num_records),\n",
    "        'shock_absorber_status': np.random.uniform(50, 100, num_records)\n",
    "    })\n",
    "\n",
    "def generate_avionics_data(num_records=100):\n",
    "    return pd.DataFrame({\n",
    "        'avionics_id': [f'avionics_{i}' for i in range(num_records)],\n",
    "        'aircraft_id': [f'A320_101' for _ in range(num_records)],\n",
    "        'power_status': np.random.uniform(110, 130, num_records),\n",
    "        'signal_integrity': np.random.uniform(20, 40, num_records),\n",
    "        'data_transmission_rate': np.random.uniform(50, 100, num_records),\n",
    "        'system_temperature': np.random.uniform(20, 50, num_records),\n",
    "        'error_logs': np.random.randint(0, 10, num_records)\n",
    "    })\n",
    "\n",
    "def generate_cabin_pressurization_data(num_records=100):\n",
    "    return pd.DataFrame({\n",
    "        'cabin_id': [f'cabin_{i}' for i in range(num_records)],\n",
    "        'aircraft_id': [f'A320_101' for _ in range(num_records)],\n",
    "        'cabin_pressure': np.random.uniform(10, 15, num_records),\n",
    "        'seal_integrity': np.random.uniform(90, 100, num_records),\n",
    "        'airflow_rate': np.random.uniform(300, 500, num_records),\n",
    "        'temperature_control': np.random.uniform(18, 25, num_records),\n",
    "        'humidity_level': np.random.uniform(20, 60, num_records)\n",
    "    })\n",
    "\n",
    "def generate_airframe_data(num_records=100):\n",
    "    return pd.DataFrame({\n",
    "        'airframe_id': [f'airframe_{i}' for i in range(num_records)],\n",
    "        'aircraft_id': [f'A320_101' for _ in range(num_records)],\n",
    "        'stress_points': np.random.uniform(100, 300, num_records),\n",
    "        'fatigue_crack_growth': np.random.uniform(0, 10, num_records),\n",
    "        'temperature_fluctuations': np.random.uniform(-30, 50, num_records),\n",
    "        'structural_integrity': np.random.uniform(50, 100, num_records)\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9f98e712-b1a4-4536-9a3b-e265dcce99c0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "def generate_engines_data(num_records=100):\n",
    "    data = {\n",
    "        'engine_id': [f'engine_{i}' for i in range(num_records)],\n",
    "        'aircraft_id': [f'A320_101' for _ in range(num_records)],  # consistent for demo\n",
    "        'manufacturer': np.random.choice(['GE', 'Rolls-Royce', 'Pratt & Whitney'], num_records),\n",
    "        'model': np.random.choice(['CFM56', 'Trent XWB', 'PW1000G'], num_records),\n",
    "        'thrust_level': np.random.uniform(50000, 120000, num_records),\n",
    "        'fuel_consumption_rate': np.random.uniform(2.0, 5.0, num_records),\n",
    "        'temperature_reading': np.random.uniform(300, 800, num_records),\n",
    "        'vibration_level': np.random.uniform(0.1, 2.0, num_records),\n",
    "        'oil_pressure': np.random.uniform(30, 80, num_records)\n",
    "    }\n",
    "    df = pd.DataFrame(data)\n",
    "    return df\n",
    "\n",
    "engines_df = generate_engines_data()\n",
    "(\n",
    "    spark.createDataFrame(engines_df)\n",
    "    .write\n",
    "    .mode(\"overwrite\")\n",
    "    .option(\"mergeSchema\", \"true\")\n",
    "    .saveAsTable(\"arao.aerodemo.engines\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f5dde128-8073-40c0-950f-48bd4c340787",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "tables = [\n",
    "    \"sensor_features\",\n",
    "    \"prediction_results\",\n",
    "    \"digital_twin_engine_view\",\n",
    "    \"digital_twin_aircraft_view\",\n",
    "    \"component_twins_master\",\n",
    "    \"twin_engine\",\n",
    "    \"twin_landing_gear\",\n",
    "    \"twin_airframe\",\n",
    "    \"twin_avionics\",\n",
    "    \"twin_cabin_pressurization\"\n",
    "]\n",
    "\n",
    "for table in tables:\n",
    "    spark.sql(f\"DROP TABLE IF EXISTS arao.aerodemo.{table}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4f9eac89-d8c2-428f-9c1e-383098ac7bdc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.table(\"arao.aerodemo.post_dlt_sanity_check\").orderBy(\"check_time\", ascending=False)\n",
    "df.display()  # or df.show(1) for latest result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7ec36a6d-c7fd-4abc-a8e4-3999e42eaa3e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "import os\n",
    "\n",
    "# Paths and tables\n",
    "engine_path = \"dbfs:/Volumes/arao/aerodemo/tmp/engine\"\n",
    "twin_engine_table = \"arao.aerodemo.twin_engine\"\n",
    "health_engine_table = \"arao.aerodemo.component_health_engine\"\n",
    "\n",
    "print(\"✅ Step 1: Checking file presence in raw engine folder...\")\n",
    "engine_files = dbutils.fs.ls(engine_path)\n",
    "if len(engine_files) == 0:\n",
    "    print(\"❌ No files found in engine data path:\", engine_path)\n",
    "else:\n",
    "    print(f\"✅ Found {len(engine_files)} files in engine data path.\")\n",
    "\n",
    "print(\"\\n✅ Step 2: Checking record count in twin_engine...\")\n",
    "try:\n",
    "    twin_count = spark.read.table(twin_engine_table).count()\n",
    "    print(f\"✔️ twin_engine row count: {twin_count}\")\n",
    "    if twin_count == 0:\n",
    "        print(\"⚠️ twin_engine table is empty. Check synthetic data generation or file format issues.\")\n",
    "except Exception as e:\n",
    "    print(\"❌ Error reading twin_engine table:\", str(e))\n",
    "\n",
    "print(\"\\n✅ Step 3: Checking record count in component_health_engine...\")\n",
    "try:\n",
    "    health_count = spark.read.table(health_engine_table).count()\n",
    "    print(f\"✔️ component_health_engine row count: {health_count}\")\n",
    "    if health_count == 0:\n",
    "        print(\"⚠️ No records in component_health_engine. Possible upstream issue.\")\n",
    "except Exception as e:\n",
    "    print(\"❌ Error reading component_health_engine table:\", str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9c753640-69d1-4317-b7a9-be1635884e58",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"/Volumes/arao/aerodemo/tmp/engine/engines_sample.csv\")\n",
    "print(df['aircraft_id'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b89a94c2-f24e-4858-86f5-ad6285215dd7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT aircraft_id, COUNT(*) \n",
    "FROM arao.aerodemo.twin_engine \n",
    "GROUP BY aircraft_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9e3ae58a-3c5f-44b4-80ea-04aa339aa8c2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT aircraft_id, COUNT(*)\n",
    "FROM arao.aerodemo.twin_engine\n",
    "GROUP BY aircraft_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2e95e60d-e7db-4726-86ed-e3638c723082",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Remove engine directory\n",
    "dbutils.fs.rm(\"/Volumes/arao/aerodemo/tmp/engine\", recurse=True)\n",
    "\n",
    "# Remove landing gear directory\n",
    "dbutils.fs.rm(\"/Volumes/arao/aerodemo/tmp/landing_gear\", recurse=True)\n",
    "\n",
    "# Remove avionics directory\n",
    "dbutils.fs.rm(\"/Volumes/arao/aerodemo/tmp/avionics\", recurse=True)\n",
    "\n",
    "# Remove cabin directory\n",
    "dbutils.fs.rm(\"/Volumes/arao/aerodemo/tmp/cabin\", recurse=True)\n",
    "\n",
    "# Remove airframe directory\n",
    "dbutils.fs.rm(\"/Volumes/arao/aerodemo/tmp/airframe\", recurse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "edaedc41-290e-47a3-9e55-06fb3c14f6af",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ✅ Check airframe CSV file columns and sample data\n",
    "\n",
    "airframe_df = (\n",
    "    spark.read\n",
    "        .format(\"csv\")\n",
    "        .option(\"header\", \"true\")\n",
    "        .load(\"/Volumes/arao/aerodemo/tmp/airframe\")\n",
    ")\n",
    "\n",
    "# Show the column names\n",
    "print(\"🔍 Columns in airframe CSV:\")\n",
    "print(airframe_df.columns)\n",
    "\n",
    "# Show a few sample rows\n",
    "print(\"\\n🔍 Sample data:\")\n",
    "airframe_df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a1f9e5a5-3a38-40dd-a9c3-39c3573de659",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ✅ Check event_timestamp in component_health_airframe DLT table\n",
    "\n",
    "health_airframe_df = spark.read.table(\"arao.aerodemo.component_health_airframe\")\n",
    "\n",
    "print(\"🔍 Columns in component_health_airframe:\")\n",
    "print(health_airframe_df.columns)\n",
    "\n",
    "print(\"\\n🔍 Sample data:\")\n",
    "health_airframe_df.select(\"aircraft_id\", \"component_id\", \"event_timestamp\", \"health_status\").show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aebaf369-0576-46f8-b3f0-e5d36e34bf82",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "DROP TABLE IF EXISTS arao.aerodemo.component_health_airframe;\n",
    "DROP TABLE IF EXISTS arao.aerodemo.component_health_landing_gear;\n",
    "DROP TABLE IF EXISTS arao.aerodemo.component_health_avionics;\n",
    "DROP TABLE IF EXISTS arao.aerodemo.component_health_cabin_pressurization;\n",
    "DROP TABLE IF EXISTS arao.aerodemo.component_health_engine;\n",
    "\n",
    "DROP TABLE IF EXISTS arao.aerodemo.digital_twin_component_view;\n",
    "DROP TABLE IF EXISTS arao.aerodemo.anomaly_alerts_component;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "155c64b3-e0e6-466a-9294-be110b04d8c1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT DISTINCT aircraft_id, base_airport_code, latitude, longitude\n",
    "FROM arao.aerodemo.aircraft_location_enriched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "422dc28a-9134-4106-8a33-c50fd3f4735d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Update A320 series\n",
    "spark.sql(\"\"\"\n",
    "    UPDATE arao.aerodemo.aircraft_airport_map\n",
    "    SET airport_code = CASE \n",
    "        WHEN aircraft_id IN ('A320_101', 'A320_102') THEN 'SFO'\n",
    "        WHEN aircraft_id = 'A320_103' THEN 'LAX'\n",
    "        WHEN aircraft_id = 'A320_104' THEN 'LAX'\n",
    "        ELSE airport_code\n",
    "    END\n",
    "\"\"\")\n",
    "\n",
    "# Update A330 series\n",
    "spark.sql(\"\"\"\n",
    "    UPDATE arao.aerodemo.aircraft_airport_map\n",
    "    SET airport_code = 'JFK'\n",
    "    WHERE aircraft_id IN ('A330_301', 'A330_302', 'A330_303')\n",
    "\"\"\")\n",
    "\n",
    "# Update B737 series\n",
    "spark.sql(\"\"\"\n",
    "    UPDATE arao.aerodemo.aircraft_airport_map\n",
    "    SET airport_code = CASE\n",
    "        WHEN aircraft_id = 'B737_201' THEN 'ORD'\n",
    "        WHEN aircraft_id = 'B737_202' THEN 'ORD'\n",
    "        WHEN aircraft_id = 'B737_203' THEN 'JFK'\n",
    "        ELSE airport_code\n",
    "    END\n",
    "\"\"\")\n",
    "\n",
    "print(\"✅ Aircraft-to-airport mappings updated!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8bc28bf3-9bd0-4f19-b104-b151f651a7ba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from databricks.sdk import WorkspaceClient\n",
    "\n",
    "w = WorkspaceClient()\n",
    "\n",
    "folder_path = \"/Workspace/Repos/anand.rao@databricks.com/databricks-aerodemo\"\n",
    "\n",
    "notebooks_to_check = [\n",
    "    # \"01_Table_Creation\",  # Optional, add when ready\n",
    "    \"02_01_Synthetic_Data_Generation_v2\",\n",
    "    \"02_02_Engine_Data_Generation\",\n",
    "    \"02_03_CabinPressurization_Data_Generation\",\n",
    "    \"02_04_Airframe_Synthetic_Data_Generation\",\n",
    "    \"02_05_LandingGear_Data_Generation\",\n",
    "    \"02_06_Avionics_Data_Generation\",\n",
    "    \"02_07_ElectricalSystems_Data_Generation\",\n",
    "    \"02_08_FuelSystems_Data_Generation\",\n",
    "    \"02_09_HydraulicSystems_Data_Generation\",\n",
    "    \"02_10_EnvironmentalSystems_Data_Generation\"\n",
    "]\n",
    "\n",
    "# Collect all notebook names in the folder\n",
    "available_notebooks = []\n",
    "for obj in w.workspace.list(path=folder_path):\n",
    "    if obj.object_type == \"NOTEBOOK\":\n",
    "        available_notebooks.append(obj.path.split(\"/\")[-1])\n",
    "\n",
    "# Debug: print all found notebooks\n",
    "print(\"📋 Notebooks found in folder:\")\n",
    "for nb in available_notebooks:\n",
    "    print(f\" - {nb}\")\n",
    "\n",
    "# Check each notebook\n",
    "for notebook in notebooks_to_check:\n",
    "    if notebook in available_notebooks:\n",
    "        print(f\"✅ Found: {notebook}\")\n",
    "    else:\n",
    "        print(f\"❌ MISSING: {notebook}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f2c97f70-cd0f-4a23-8696-cdf7db964745",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from databricks.sdk import WorkspaceClient\n",
    "\n",
    "w = WorkspaceClient()\n",
    "\n",
    "for obj in w.workspace.list(path=\"/Workspace/Repos\"):\n",
    "    print(f\"{obj.object_type}: {obj.path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "18300d2d-3b0b-464b-83da-074e768238b2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "notebook_path = dbutils.notebook.entry_point.getDbutils().notebook().getContext().notebookPath().get()\n",
    "print(f\"📍 Current notebook path: {notebook_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aeac0042-537f-4c9d-a18c-2ad5a7f7e3ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "notebook_path = dbutils.notebook.entry_point.getDbutils().notebook().getContext().notebookPath().get()\n",
    "folder_path = os.path.dirname(notebook_path)\n",
    "\n",
    "print(f\"✅ Detected notebook folder: {folder_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dc3211d0-fe46-43a9-895f-5ceceb17c794",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from databricks.sdk import WorkspaceClient\n",
    "\n",
    "w = WorkspaceClient()\n",
    "\n",
    "job_id = \"173822373344591\"\n",
    "\n",
    "job = w.jobs.get(job_id)\n",
    "\n",
    "print(f\"✅ Job name: {job.settings.name}\")\n",
    "print(\"✅ Registered tasks:\")\n",
    "\n",
    "for task in job.settings.tasks:\n",
    "    task_name = task.task_key\n",
    "    notebook_path = task.notebook_task.notebook_path if task.notebook_task else \"N/A\"\n",
    "    print(f\" - {task_name}: {notebook_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c84db141-5a99-4616-85f7-260b12aedbbb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "job_id = \"173822373344591\"\n",
    "# Retrieve the job details\n",
    "retrieved_job = w.jobs.get(job_id)\n",
    "\n",
    "print(f\"✅ Job '{retrieved_job.settings.name}' has the following tasks:\")\n",
    "for task in retrieved_job.settings.tasks:\n",
    "    if task.notebook_task:\n",
    "        print(f\" - Notebook task: {task.task_key} → {task.notebook_task.notebook_path}\")\n",
    "    if task.pipeline_task:\n",
    "        print(f\" - DLT pipeline task: {task.task_key} → Pipeline ID: {task.pipeline_task.pipeline_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "75088201-4369-414e-ae1d-7d2f3f8bfc3d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ✅ Check 1: Unique airport locations\n",
    "unique_airports = spark.sql(\"\"\"\n",
    "    SELECT DISTINCT latitude, longitude \n",
    "    FROM arao.aerodemo.aircraft_location_enriched_v2\n",
    "\"\"\").count()\n",
    "print(f\"✅ Unique airport locations: {unique_airports}\")\n",
    "\n",
    "# ✅ Check 2: Unique aircraft IDs with CRITICAL or WARNING alerts\n",
    "unique_aircraft_alerts = spark.sql(\"\"\"\n",
    "    SELECT DISTINCT aircraft_id \n",
    "    FROM arao.aerodemo.anomaly_alerts_component\n",
    "    WHERE health_status IN ('CRITICAL', 'WARNING')\n",
    "\"\"\").count()\n",
    "print(f\"✅ Unique aircraft with CRITICAL/WARNING alerts: {unique_aircraft_alerts}\")\n",
    "\n",
    "# ✅ Check 3: Aircraft with most CRITICAL or WARNING alerts\n",
    "aircraft_alerts = spark.sql(\"\"\"\n",
    "    SELECT aircraft_id, COUNT(*) AS alert_count\n",
    "    FROM arao.aerodemo.anomaly_alerts_component\n",
    "    WHERE health_status IN ('CRITICAL', 'WARNING')\n",
    "    GROUP BY aircraft_id\n",
    "    ORDER BY alert_count DESC\n",
    "\"\"\").toPandas()\n",
    "\n",
    "if not aircraft_alerts.empty:\n",
    "    print(\"✅ Top aircraft by alert count:\")\n",
    "    print(aircraft_alerts.head(10))\n",
    "else:\n",
    "    print(\"⚠️ No CRITICAL or WARNING alerts found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c0ce023c-0998-4183-84e6-9f39abdefcee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 1️⃣ Check distinct aircraft IDs per model\n",
    "df_aircraft_count = spark.sql(\"\"\"\n",
    "    SELECT model, COUNT(DISTINCT aircraft_id) AS num_aircraft\n",
    "    FROM arao.aerodemo.sensor_raw_v2\n",
    "    GROUP BY model\n",
    "    ORDER BY model\n",
    "\"\"\").toPandas()\n",
    "print(\"✅ Aircraft count per model:\")\n",
    "print(df_aircraft_count)\n",
    "\n",
    "# 2️⃣ Check date range in the raw data\n",
    "df_date_range = spark.sql(\"\"\"\n",
    "    SELECT MIN(timestamp) AS start_date, MAX(timestamp) AS end_date\n",
    "    FROM arao.aerodemo.sensor_raw_v2\n",
    "\"\"\").toPandas()\n",
    "print(\"\\n✅ Date range in raw data:\")\n",
    "print(df_date_range)\n",
    "\n",
    "# 3️⃣ Check the distribution of anomaly scores\n",
    "df_anomaly_dist = spark.sql(\"\"\"\n",
    "    SELECT anomaly_score, COUNT(*) AS count\n",
    "    FROM arao.aerodemo.sensor_raw_v2\n",
    "    GROUP BY anomaly_score\n",
    "    ORDER BY anomaly_score DESC\n",
    "\"\"\").toPandas()\n",
    "print(\"\\n✅ Anomaly score distribution:\")\n",
    "print(df_anomaly_dist)\n",
    "\n",
    "# 4️⃣ Check if post-repair data resets metrics\n",
    "df_post_repair = spark.sql(\"\"\"\n",
    "    SELECT aircraft_id, AVG(engine_temp) AS avg_temp_after_repair\n",
    "    FROM arao.aerodemo.sensor_raw_v2\n",
    "    WHERE anomaly_score = 0.0  -- After repair\n",
    "    GROUP BY aircraft_id\n",
    "    ORDER BY avg_temp_after_repair DESC\n",
    "    LIMIT 10\n",
    "\"\"\").toPandas()\n",
    "print(\"\\n✅ Top 10 aircraft by average engine temp (after repair):\")\n",
    "print(df_post_repair)\n",
    "\n",
    "# 5️⃣ Check maintenance events coverage\n",
    "df_maintenance = spark.sql(\"\"\"\n",
    "    SELECT event_type, COUNT(*) AS num_events\n",
    "    FROM arao.aerodemo.maintenance_events_v2\n",
    "    GROUP BY event_type\n",
    "\"\"\").toPandas()\n",
    "print(\"\\n✅ Maintenance events breakdown:\")\n",
    "print(df_maintenance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a3aafce3-0400-443f-8d26-0ed758a5916a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# List all tables in the catalog + schema\n",
    "df_tables = spark.sql(\"\"\"\n",
    "    SHOW TABLES IN arao.aerodemo\n",
    "\"\"\").toPandas()\n",
    "\n",
    "print(\"✅ Available tables in arao.aerodemo schema:\")\n",
    "print(df_tables[['tableName', 'isTemporary']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1cc99dea-ea3d-4cc1-9092-6162f2efe725",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# List files in the raw sensor input folder (adjust path as needed)\n",
    "input_path = \"dbfs:/mnt/aerodemo/raw_sensor_data/\"  # ← replace with your actual raw folder\n",
    "\n",
    "files = dbutils.fs.ls(input_path)\n",
    "\n",
    "print(f\"✅ Found {len(files)} files in {input_path}\")\n",
    "for f in files:\n",
    "    print(f.name, f.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3c6a7fae-97f0-4c5c-b045-8d15afdc9fa5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.fs.ls(\"/Volumes/arao/aerodemo/tmp/raw/\")\n",
    "dbutils.fs.ls(\"/Volumes/arao/aerodemo/tmp/maintenance/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1e26a811-4e63-441a-9d17-579de758b124",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Use Spark to read from DBFS (avoids local mount issues)\n",
    "spark_df = spark.read.option(\"header\", True).csv(latest_file_path)\n",
    "\n",
    "# Convert to Pandas\n",
    "latest_df = spark_df.toPandas()\n",
    "\n",
    "print(\"✅ Loaded DataFrame:\")\n",
    "print(latest_df.head())\n",
    "\n",
    "print(f\"✅ Total rows: {len(latest_df)}\")\n",
    "print(f\"✅ Unique aircraft IDs: {latest_df['aircraft_id'].nunique()}\")\n",
    "print(f\"✅ Unique event types: {latest_df['event_type'].unique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "372a0cd5-7085-4bdf-9934-a3b5d5045dbc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "latest_df['event_type'].value_counts()\n",
    "latest_df.groupby('aircraft_id').size().sort_values(ascending=False)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "latest_df['event_date'] = pd.to_datetime(latest_df['event_date'])\n",
    "latest_df['event_date'].hist(bins=20)\n",
    "plt.title(\"Event Dates Distribution\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "86be0855-f329-4902-afc9-158ad6e72c93",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Check 1: Airport reference\n",
    "print(\"✅ Airport reference table sample:\")\n",
    "print(df_airport[['airport_code', 'airport_name', 'latitude', 'longitude']].head())\n",
    "\n",
    "# Check 2: Aircraft-airport map\n",
    "df_aircraft_airport = spark.sql(\"SELECT * FROM arao.aerodemo.aircraft_airport_map\").toPandas()\n",
    "print(f\"\\n✅ Aircraft-airport map: {len(df_aircraft_airport)} records\")\n",
    "print(f\"Available columns: {df_aircraft_airport.columns.tolist()}\")\n",
    "print(df_aircraft_airport.head())\n",
    "\n",
    "# Check 3: Enriched location table\n",
    "df_location = spark.sql(\"\"\"\n",
    "    SELECT DISTINCT aircraft_id, airport_code, latitude, longitude\n",
    "    FROM arao.aerodemo.aircraft_location_enriched_v2\n",
    "\"\"\").toPandas()\n",
    "print(f\"\\n✅ Enriched aircraft locations: {len(df_location)} distinct aircraft-airport pairs\")\n",
    "print(df_location.head())\n",
    "\n",
    "# Check 4: Alerting aircraft\n",
    "df_alerts = spark.sql(\"\"\"\n",
    "    SELECT DISTINCT aircraft_id\n",
    "    FROM arao.aerodemo.anomaly_alerts_component\n",
    "    WHERE health_status IN ('CRITICAL', 'WARNING')\n",
    "\"\"\").toPandas()\n",
    "print(f\"\\n✅ Aircraft triggering CRITICAL/WARNING alerts: {len(df_alerts)} aircraft\")\n",
    "print(df_alerts.head())\n",
    "\n",
    "# Check 5: Join alerts with location\n",
    "df_alerts_locations = df_alerts.merge(df_location, on='aircraft_id', how='left')\n",
    "missing_airports = df_alerts_locations[df_alerts_locations['airport_code'].isna()]\n",
    "if not missing_airports.empty:\n",
    "    print(\"\\n⚠️ ALERT: Some alerting aircraft have no airport mapping!\")\n",
    "    print(missing_airports)\n",
    "else:\n",
    "    print(\"\\n✅ All alerting aircraft have airport mappings.\")\n",
    "\n",
    "print(\"\\n✅ Summary of alerting aircraft by airport:\")\n",
    "print(df_alerts_locations['airport_code'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d180c2d3-3dec-4c9e-b99f-f2b64014fcdc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Check 1: Airport reference — first, show columns\n",
    "df_airport = spark.sql(\"SELECT * FROM arao.aerodemo.airport_reference\").toPandas()\n",
    "print(f\"✅ Airport reference table: {len(df_airport)} records\")\n",
    "print(f\"Available columns: {df_airport.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4d2ca7d3-5c92-45a6-8dfa-ead344bcd785",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Check if these aircraft exist in synthetic data\n",
    "print(\"✅ Checking if alerting aircraft exist in synthetic aircraft list...\")\n",
    "print(df_aircraft_airport[df_aircraft_airport['aircraft_id'].isin(df_alerts['aircraft_id'])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e9246884-4ea4-4b87-bc0e-73d1b99ba99e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "df_alerts = spark.sql(\"SELECT DISTINCT aircraft_id FROM arao.aerodemo.anomaly_alerts_component\").toPandas()\n",
    "df_aircraft_airport = spark.sql(\"SELECT * FROM arao.aerodemo.aircraft_airport_map\").toPandas()\n",
    "\n",
    "merged = df_alerts.merge(df_aircraft_airport, on=\"aircraft_id\", how=\"left\")\n",
    "missing_airports = merged[merged['airport_code'].isnull()]\n",
    "\n",
    "if missing_airports.empty:\n",
    "    print(\"✅ All alerting aircraft now have airport mappings!\")\n",
    "else:\n",
    "    print(\"⚠ Still missing airport mappings for:\")\n",
    "    print(missing_airports)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 2588019255063078,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "Scratch Notebook",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
