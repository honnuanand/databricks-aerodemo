{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8811c6cc-2d59-42e2-92cb-d80e76b61b71",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# ðŸ“¦ Step 1: Create Delta Table for Centralized Config Store\n",
    "\n",
    "This notebook sets up a Delta table (`aerodemo_config_store`) to hold key-value configuration data \n",
    "for the AeroDemo project. \n",
    "\n",
    "âœ… This will replace the need to hardcode or edit Python files to track things like workflow job IDs,  \n",
    "DLT pipeline IDs, cluster settings, and environment-specific toggles.\n",
    "\n",
    "We'll create the table once under the shared catalog and schema, and then manage all dynamic configs \n",
    "there from other notebooks and workflows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5ee97c3c-25e4-46cf-b21a-3f3a6cb4fff6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### What this notebook does:\n",
    "\n",
    "1. Defines the **catalog** and **schema** for the table location.\n",
    "2. Defines a Delta table `aerodemo_config_store` with columns:\n",
    "   - `env`: environment name (e.g., dev, staging, prod)\n",
    "   - `config_key`: name of the config (e.g., e2e_workflow_job_id)\n",
    "   - `config_value`: value of the config\n",
    "3. Overwrites any existing table with a clean schema.\n",
    "4. Optionally inserts initial config rows for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7ae52240-7af4-480f-b22f-0018b39ab7e5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Import Spark session\n",
    "spark = spark\n",
    "\n",
    "# Set catalog and schema (match your dev environment)\n",
    "CATALOG = \"arao\"\n",
    "SCHEMA = \"aerodemo\"\n",
    "\n",
    "TABLE_NAME = f\"{CATALOG}.{SCHEMA}.aerodemo_config_store\"\n",
    "\n",
    "print(f\"âœ… Creating table: {TABLE_NAME}\")\n",
    "\n",
    "# Drop if exists\n",
    "spark.sql(f\"DROP TABLE IF EXISTS {TABLE_NAME}\")\n",
    "\n",
    "# Create new Delta table\n",
    "spark.sql(f\"\"\"\n",
    "    CREATE TABLE {TABLE_NAME} (\n",
    "        env STRING,\n",
    "        config_key STRING,\n",
    "        config_value STRING\n",
    "    )\n",
    "    USING DELTA\n",
    "    COMMENT 'Key-Value configuration store for AeroDemo project'\n",
    "\"\"\")\n",
    "\n",
    "print(f\"âœ… Table {TABLE_NAME} created successfully!\")\n",
    "\n",
    "# Optional: Insert sample configs\n",
    "sample_data = [\n",
    "    (\"dev\", \"e2e_workflow_job_id\", \"123456789\"),\n",
    "    (\"dev\", \"dlt_pipeline_id\", \"dev-dlt-id\"),\n",
    "    (\"dev\", \"existing_cluster_id\", \"dev-cluster-id\")\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(sample_data, [\"env\", \"config_key\", \"config_value\"])\n",
    "df.write.format(\"delta\").mode(\"append\").saveAsTable(TABLE_NAME)\n",
    "\n",
    "print(f\"âœ… Sample data inserted into {TABLE_NAME}\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "01_Create_Config_Store",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
