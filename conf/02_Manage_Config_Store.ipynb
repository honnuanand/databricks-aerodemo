{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "274b1754-76e6-4923-a55e-df9245637b6c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# ðŸ“Š 02_Manage_Config_Store\n",
    "\n",
    "This notebook allows you to **safely insert or update (upsert)** configuration key-value pairs  \n",
    "into the `aerodemo_config_store` Delta table without dropping or recreating it.\n",
    "\n",
    "âœ… Use this notebook to:\n",
    "- Add new configuration keys.\n",
    "- Update existing configuration values.\n",
    "- Manage environment-specific configs (`dev`, `staging`, `prod`).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "45a946d1-978f-4632-958d-ca507a5ba58f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## ðŸ”§ Upsert Single Config Key-Value\n",
    "\n",
    "The cell below will:\n",
    "- Define the target environment (`env`).\n",
    "- Provide the `config_key` and new `config_value`.\n",
    "- Perform a Delta Lake `MERGE` (safe upsert) into the config store.\n",
    "\n",
    "Make sure to update the variables (`env`, `config_key`, `config_value`) as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "88aa4b98-8cd3-4903-8697-8a5c28d43c14",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Import Spark session\n",
    "spark = spark\n",
    "\n",
    "# Set catalog and schema\n",
    "CATALOG = \"arao\"\n",
    "SCHEMA = \"aerodemo\"\n",
    "TABLE_NAME = f\"{CATALOG}.{SCHEMA}.aerodemo_config_store\"\n",
    "\n",
    "# Target values to upsert\n",
    "env = \"dev\"\n",
    "config_key = \"e2e_workflow_job_id\"\n",
    "config_value = \"NEW_JOB_ID_999999\"\n",
    "\n",
    "print(f\"âœ… Upserting â†’ env: {env}, key: {config_key}, value: {config_value}\")\n",
    "\n",
    "# Create single-row DataFrame for the upsert\n",
    "from pyspark.sql import Row\n",
    "\n",
    "new_row = [Row(env=env, config_key=config_key, config_value=config_value)]\n",
    "df = spark.createDataFrame(new_row)\n",
    "\n",
    "# Perform MERGE (upsert)`\n",
    "spark.sql(f\"\"\"\n",
    "MERGE INTO {TABLE_NAME} AS target\n",
    "USING (SELECT '{env}' AS env, '{config_key}' AS config_key, '{config_value}' AS config_value) AS source\n",
    "ON target.env = source.env AND target.config_key = source.config_key\n",
    "WHEN MATCHED THEN UPDATE SET target.config_value = source.config_value\n",
    "WHEN NOT MATCHED THEN INSERT (env, config_key, config_value) VALUES (source.env, source.config_key, source.config_value)\n",
    "\"\"\")\n",
    "\n",
    "print(f\"âœ… Successfully upserted {config_key} for environment '{env}' into {TABLE_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "491bffd8-7024-4bf5-9780-c81ff7a3e0e9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "> ðŸ’¡ Tip: You can copy this cell and change the `env`, `config_key`, and `config_value`  \n",
    "> to manage multiple configs as needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "038ff809-e799-4cc9-88e0-11268ee2d879",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## ðŸ”§ Batch Upsert Multiple Configs\n",
    "\n",
    "This cell will:\n",
    "- Take a list of `(env, config_key, config_value)` entries.\n",
    "- Convert them into a DataFrame.\n",
    "- Perform a Delta Lake `MERGE` to upsert all keys into the config table **in one shot**.\n",
    "\n",
    "This avoids looping row by row and is more efficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d00cb0cd-ea3d-46f5-9b95-378f3d3b3551",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "# Full CONFIG dictionary (same as before)\n",
    "CONFIG = {\n",
    "    \"dev\": {\n",
    "        \"catalog\": \"arao\",\n",
    "        \"schema\": \"aerodemo\",\n",
    "        \"pat_token\": \"YOUR_DEV_PAT_TOKEN\",\n",
    "        \"databricks_instance\": \"https://e2-demo-field-eng.cloud.databricks.com/\",\n",
    "        \"e2e_workflow_job_id\": \"962701319088715\",\n",
    "        \"workflow_configs\": {\n",
    "            \"workflow_name\": \"AeroDemo End to End Workflow\",\n",
    "            \"existing_cluster_id\": \"0527-220936-f3oreeiv\",\n",
    "            \"dlt_pipeline_id\": \"a2ccd850-4b28-4f30-9a53-0fd5f5499713\"\n",
    "        }\n",
    "    },\n",
    "    \"staging\": {\n",
    "        \"catalog\": \"arao_staging\",\n",
    "        \"schema\": \"aerodemo_staging\",\n",
    "        \"pipeline_ids\": {\n",
    "            \"full_pipeline\": \"staging-pipeline-id-here\",\n",
    "            \"registration_pipeline\": \"staging-registration-pipeline-id-here\"\n",
    "        },\n",
    "        \"pat_token\": \"dapi-STAGING-XXXXXX\",\n",
    "        \"databricks_instance\": \"https://e2-demo-field-eng.cloud.databricks.com/\",\n",
    "        \"e2e_workflow_job_id\": \"staging-864722071013094\",\n",
    "        \"workflow_configs\": {\n",
    "            \"workflow_name\": \"AeroDemo_DataPipeline_Staging\",\n",
    "            \"existing_cluster_id\": \"staging-cluster-id\",\n",
    "            \"dlt_pipeline_id\": \"staging-dlt-pipeline-id\"\n",
    "        }\n",
    "    },\n",
    "    \"prod\": {\n",
    "        \"catalog\": \"arao\",\n",
    "        \"schema\": \"aerodemo\",\n",
    "        \"pipeline_ids\": {\n",
    "            \"full_pipeline\": \"prod-pipeline-id-here\",\n",
    "            \"registration_pipeline\": \"prod-registration-pipeline-id-here\"\n",
    "        },\n",
    "        \"pat_token\": \"dapi-PROD-XXXXXX\",\n",
    "        \"databricks_instance\": \"https://e2-demo-field-eng.cloud.databricks.com/\",\n",
    "        \"e2e_workflow_job_id\": \"864722071013094\",\n",
    "        \"workflow_configs\": {\n",
    "            \"workflow_name\": \"AeroDemo_DataPipeline_Prod\",\n",
    "            \"existing_cluster_id\": \"prod-cluster-id\",\n",
    "            \"dlt_pipeline_id\": \"prod-dlt-pipeline-id\"\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Flatten configs into (env, key, value) triples\n",
    "configs_to_upsert = []\n",
    "\n",
    "def flatten_config(env, d, parent_key=\"\"):\n",
    "    for k, v in d.items():\n",
    "        new_key = f\"{parent_key}.{k}\" if parent_key else k\n",
    "        if isinstance(v, dict):\n",
    "            flatten_config(env, v, new_key)\n",
    "        else:\n",
    "            configs_to_upsert.append((env, new_key, str(v)))\n",
    "\n",
    "for env, config_dict in CONFIG.items():\n",
    "    flatten_config(env, config_dict)\n",
    "\n",
    "print(f\"ðŸ” Prepared {len(configs_to_upsert)} flattened configs for upsert.\")\n",
    "\n",
    "# Set catalog, schema, and table name\n",
    "CATALOG = \"arao\"\n",
    "SCHEMA = \"aerodemo\"\n",
    "TABLE_NAME = f\"{CATALOG}.{SCHEMA}.aerodemo_config_store\"\n",
    "\n",
    "# âœ… Optional: Clear all configs first\n",
    "CLEAR_BEFORE_INSERT = True\n",
    "\n",
    "if CLEAR_BEFORE_INSERT:\n",
    "    spark.sql(f\"DELETE FROM {TABLE_NAME}\")\n",
    "    print(f\"âš ï¸ Cleared all existing configs from {TABLE_NAME}\")\n",
    "\n",
    "# Convert list to DataFrame\n",
    "df = spark.createDataFrame([Row(env=env, config_key=key, config_value=value) for env, key, value in configs_to_upsert])\n",
    "\n",
    "# Perform MERGE (upsert)\n",
    "temp_view = \"temp_config_upserts\"\n",
    "df.createOrReplaceTempView(temp_view)\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "MERGE INTO {TABLE_NAME} AS target\n",
    "USING {temp_view} AS source\n",
    "ON target.env = source.env AND target.config_key = source.config_key\n",
    "WHEN MATCHED THEN UPDATE SET target.config_value = source.config_value\n",
    "WHEN NOT MATCHED THEN INSERT (env, config_key, config_value) VALUES (source.env, source.config_key, source.config_value)\n",
    "\"\"\")\n",
    "\n",
    "print(f\"âœ… Successfully upserted {len(configs_to_upsert)} configs into {TABLE_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "072c4068-273b-4338-a38a-59cb0ec18361",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "> ðŸ’¡ You can update the `configs_to_upsert` list with as many key-value pairs  \n",
    "> as you want across different environments."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "02_Manage_Config_Store",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
